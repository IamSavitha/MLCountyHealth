{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4MTWE1oAdcEWY0Yj0rQT2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install plotly"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9Uzm6WJhHqT","executionInfo":{"status":"ok","timestamp":1763837987919,"user_tz":480,"elapsed":11914,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"36da316f-b335-4ea2-c45a-93cb414c8c87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qOv8wlP7aQM2","executionInfo":{"status":"ok","timestamp":1763838135243,"user_tz":480,"elapsed":33297,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"5d716e0c-0c86-498d-883c-e2dbd5a34d00"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","FOOD DESERT AND HEALTH RISK ANALYSIS\n","======================================================================\n","\n","STEP 0: DATA PREPARATION\n","======================================================================\n","\n","[1/8] Loading data...\n","Mounted at /content/drive\n","Loaded: 2275 counties, 45 features\n","\n","[2/8] Fixing FIPS format...\n","\n","[3/8] Creating target variables...\n","Three-class: Low<0.264, Medium=0.264-0.287, High>0.287\n","Binary threshold: 38.7% obesity\n","\n","[4/8] Handling missing values...\n","After cleaning: 2236 counties\n","\n","[5/8] Defining feature sets...\n","Feature Set 1: 11 features\n","Note: Removed health outcome indicators to avoid data leakage\n","\n","[6/8] Creating PCA features...\n","PCA variance explained: 91.5%\n","Feature Set 2 (PCA): 6 features\n","Feature Set 3 (Simplified): 6 features\n","\n","[7/8] Splitting train/test sets...\n","Train set: 1565 counties\n","Test set: 671 counties\n","\n","[8/8] Saving prepared data...\n","Data preparation complete!\n"]}],"source":["\"\"\"\n","Food Desert and Health Risk Analysis\n","Imports and Data Preparation\n","\"\"\"\n","\n","# Imports\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib\n","matplotlib.use('Agg')\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler, label_binarize\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import DBSCAN\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import (\n","    RandomForestClassifier,\n","    ExtraTreesClassifier,\n","    GradientBoostingClassifier,\n","    BaggingClassifier,\n","    VotingClassifier,\n","    StackingClassifier\n",")\n","from sklearn.metrics import (\n","    classification_report, confusion_matrix,\n","    accuracy_score, f1_score, cohen_kappa_score,\n","    roc_auc_score\n",")\n","import plotly.graph_objects as go\n","import time\n","\n","print(\"=\"*70)\n","print(\"FOOD DESERT AND HEALTH RISK ANALYSIS\")\n","print(\"=\"*70)\n","\n","# ============================================================================\n","# STEP 0: DATA PREPARATION\n","# ============================================================================\n","\n","print(\"\\nSTEP 0: DATA PREPARATION\")\n","print(\"=\"*70)\n","\n","# Load data (user must upload cleaned_health_data.csv first)\n","print(\"\\n[1/8] Loading data...\")\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","df = pd.read_csv('/content/drive/MyDrive/cleaned_health_data.csv')\n","print(f\"Loaded: {len(df)} counties, {len(df.columns)} features\")\n","\n","# Fix FIPS format\n","print(\"\\n[2/8] Fixing FIPS format...\")\n","df['FIPS'] = df['FIPS'].astype(str).str.zfill(5)\n","\n","# Create target variables\n","print(\"\\n[3/8] Creating target variables...\")\n","\n","# Three-class target\n","risk_low = df['Health_Risk_Score'].quantile(0.33)\n","risk_high = df['Health_Risk_Score'].quantile(0.67)\n","df['Risk_Category_3Class'] = pd.cut(\n","    df['Health_Risk_Score'],\n","    bins=[-np.inf, risk_low, risk_high, np.inf],\n","    labels=['Low', 'Medium', 'High']\n",")\n","\n","# Binary target\n","obesity_median = df['% Adults with Obesity'].median()\n","df['Risk_Category_Binary'] = (df['% Adults with Obesity'] >= obesity_median).map({\n","    True: 'High', False: 'Low'\n","})\n","\n","print(f\"Three-class: Low<{risk_low:.3f}, Medium={risk_low:.3f}-{risk_high:.3f}, High>{risk_high:.3f}\")\n","print(f\"Binary threshold: {obesity_median:.1f}% obesity\")\n","\n","# Handle missing values\n","print(\"\\n[4/8] Handling missing values...\")\n","key_columns = [\n","    'FIPS', 'State', 'County',\n","    'Risk_Category_3Class', 'Risk_Category_Binary',\n","    'Food_Access_Barrier_Index', 'Food Environment Index',\n","    '80th Percentile Income', '20th Percentile Income', 'Income Ratio',\n","    '% Children in Poverty', '% Some College', '% Completed High School',\n","    '% Uninsured', '% Rural', 'High_Income_Inequality',\n","    '% Adults with Obesity', '% Adults with Diabetes'\n","]\n","\n","df_clean = df.dropna(subset=key_columns)\n","print(f\"After cleaning: {len(df_clean)} counties\")\n","\n","# Define features - REMOVED \"Average Number of Physically Unhealthy Days\"\n","print(\"\\n[5/8] Defining feature sets...\")\n","\n","features_set1 = [\n","    'Food_Access_Barrier_Index',\n","    'Food Environment Index',\n","    '80th Percentile Income',\n","    '20th Percentile Income',\n","    'Income Ratio',\n","    '% Children in Poverty',\n","    '% Some College',\n","    '% Completed High School',\n","    '% Uninsured',\n","    '% Rural',\n","    'High_Income_Inequality'\n","]\n","\n","print(f\"Feature Set 1: {len(features_set1)} features\")\n","print(\"Note: Removed health outcome indicators to avoid data leakage\")\n","\n","# PCA features\n","print(\"\\n[6/8] Creating PCA features...\")\n","socioeconomic_features = [\n","    '80th Percentile Income',\n","    '20th Percentile Income',\n","    'Income Ratio',\n","    '% Children in Poverty',\n","    '% Some College',\n","    '% Completed High School'\n","]\n","\n","X_pca = df_clean[socioeconomic_features]\n","scaler_pca = StandardScaler()\n","X_scaled_pca = scaler_pca.fit_transform(X_pca)\n","\n","pca = PCA(n_components=3)\n","pca_components = pca.fit_transform(X_scaled_pca)\n","\n","df_clean['PC1'] = pca_components[:, 0]\n","df_clean['PC2'] = pca_components[:, 1]\n","df_clean['PC3'] = pca_components[:, 2]\n","\n","print(f\"PCA variance explained: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n","\n","features_set2 = [\n","    'PC1', 'PC2', 'PC3',\n","    'Food_Access_Barrier_Index',\n","    'Food Environment Index',\n","    '% Rural'\n","]\n","\n","features_set3 = [\n","    'Food_Access_Barrier_Index',\n","    'Income Ratio',\n","    '% Children in Poverty',\n","    '% Completed High School',\n","    '% Rural',\n","    'High_Income_Inequality'\n","]\n","\n","print(f\"Feature Set 2 (PCA): {len(features_set2)} features\")\n","print(f\"Feature Set 3 (Simplified): {len(features_set3)} features\")\n","\n","# Prepare data\n","print(\"\\n[7/8] Splitting train/test sets...\")\n","X1 = df_clean[features_set1]\n","X2 = df_clean[features_set2]\n","X3 = df_clean[features_set3]\n","y_3class = df_clean['Risk_Category_3Class']\n","y_binary = df_clean['Risk_Category_Binary']\n","\n","# Three-class splits\n","X1_train_3c, X1_test_3c, y_train_3c, y_test_3c = train_test_split(\n","    X1, y_3class, test_size=0.3, random_state=42, stratify=y_3class\n",")\n","\n","X2_train_3c, X2_test_3c, _, _ = train_test_split(\n","    X2, y_3class, test_size=0.3, random_state=42, stratify=y_3class\n",")\n","\n","X3_train_3c, X3_test_3c, _, _ = train_test_split(\n","    X3, y_3class, test_size=0.3, random_state=42, stratify=y_3class\n",")\n","\n","# Binary splits\n","X1_train_bin, X1_test_bin, y_train_bin, y_test_bin = train_test_split(\n","    X1, y_binary, test_size=0.3, random_state=42, stratify=y_binary\n",")\n","\n","X2_train_bin, X2_test_bin, _, _ = train_test_split(\n","    X2, y_binary, test_size=0.3, random_state=42, stratify=y_binary\n",")\n","\n","X3_train_bin, X3_test_bin, _, _ = train_test_split(\n","    X3, y_binary, test_size=0.3, random_state=42, stratify=y_binary\n",")\n","\n","print(f\"Train set: {len(X1_train_3c)} counties\")\n","print(f\"Test set: {len(X1_test_3c)} counties\")\n","\n","# Save prepared data\n","print(\"\\n[8/8] Saving prepared data...\")\n","df_clean.to_csv('data_clean_final.csv', index=False)\n","print(\"Data preparation complete!\")\n"]},{"cell_type":"code","source":["\"\"\"\n","Step 1 Three-Class Classification with Gini Index Analysis\n","\"\"\"\n","\n","# ============================================================================\n","# STEP 1: THREE-CLASS CLASSIFICATION + GINI INDEX\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"STEP 1: THREE-CLASS CLASSIFICATION + GINI INDEX\")\n","print(\"=\"*70)\n","\n","step1_results = []\n","\n","feature_sets_3c = {\n","    'Set1_Original': (X1_train_3c, X1_test_3c),\n","    'Set2_PCA': (X2_train_3c, X2_test_3c),\n","    'Set3_Simplified': (X3_train_3c, X3_test_3c)\n","}\n","\n","print(\"\\n[1/4] Training models...\")\n","\n","for set_name, (X_tr, X_te) in feature_sets_3c.items():\n","    print(f\"\\n{set_name} ({X_tr.shape[1]} features)...\")\n","\n","    # Random Forest\n","    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n","    rf_model.fit(X_tr, y_train_3c)\n","    rf_pred = rf_model.predict(X_te)\n","    rf_proba = rf_model.predict_proba(X_te)\n","\n","    # Extra Trees\n","    et_model = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n","    et_model.fit(X_tr, y_train_3c)\n","    et_pred = et_model.predict(X_te)\n","    et_proba = et_model.predict_proba(X_te)\n","\n","    # Calculate metrics\n","    y_test_bin_3c = label_binarize(y_test_3c, classes=['Low', 'Medium', 'High'])\n","    rf_auc = roc_auc_score(y_test_bin_3c, rf_proba, average='weighted', multi_class='ovr')\n","    et_auc = roc_auc_score(y_test_bin_3c, et_proba, average='weighted', multi_class='ovr')\n","\n","    step1_results.append({\n","        'Feature_Set': set_name,\n","        'Model': 'Random Forest',\n","        'Accuracy': accuracy_score(y_test_3c, rf_pred),\n","        'F1_Weighted': f1_score(y_test_3c, rf_pred, average='weighted'),\n","        'F1_Macro': f1_score(y_test_3c, rf_pred, average='macro'),\n","        'Cohens_Kappa': cohen_kappa_score(y_test_3c, rf_pred),\n","        'ROC_AUC': rf_auc\n","    })\n","\n","    step1_results.append({\n","        'Feature_Set': set_name,\n","        'Model': 'Extra Trees',\n","        'Accuracy': accuracy_score(y_test_3c, et_pred),\n","        'F1_Weighted': f1_score(y_test_3c, et_pred, average='weighted'),\n","        'F1_Macro': f1_score(y_test_3c, et_pred, average='macro'),\n","        'Cohens_Kappa': cohen_kappa_score(y_test_3c, et_pred),\n","        'ROC_AUC': et_auc\n","    })\n","\n","    if set_name == 'Set1_Original':\n","        best_rf_3c = rf_model\n","        best_et_3c = et_model\n","        best_rf_pred_3c = rf_pred\n","        best_et_pred_3c = et_pred\n","\n","# Save results\n","step1_df = pd.DataFrame(step1_results)\n","step1_df.to_csv('step1_model_comparison_results.csv', index=False)\n","\n","print(f\"\\nRandom Forest: {step1_df[(step1_df['Model']=='Random Forest') & (step1_df['Feature_Set']=='Set1_Original')]['Accuracy'].values[0]:.4f}\")\n","print(f\"Extra Trees: {step1_df[(step1_df['Model']=='Extra Trees') & (step1_df['Feature_Set']=='Set1_Original')]['Accuracy'].values[0]:.4f}\")\n","\n","# Confusion matrices\n","print(\"\\n[2/4] Generating confusion matrices...\")\n","cm_rf_3c = confusion_matrix(y_test_3c, best_rf_pred_3c, labels=['High', 'Low', 'Medium'])\n","cm_et_3c = confusion_matrix(y_test_3c, best_et_pred_3c, labels=['High', 'Low', 'Medium'])\n","\n","np.savetxt('step1_confusion_matrix_rf.csv', cm_rf_3c, delimiter=',', fmt='%d')\n","np.savetxt('step1_confusion_matrix_et.csv', cm_et_3c, delimiter=',', fmt='%d')\n","\n","# Gini Index Calculation\n","print(\"\\n[3/4] Calculating Gini Index...\")\n","\n","def calculate_gini_index(y_true):\n","    \"\"\"\n","    Calculate Gini impurity for a label array\n","    Gini = 1 - sum(p_i^2) where p_i is proportion of class i\n","    Lower Gini = more pure classes\n","    \"\"\"\n","    value_counts = pd.Series(y_true).value_counts()\n","    proportions = value_counts / len(y_true)\n","    gini = 1 - (proportions ** 2).sum()\n","    return gini\n","\n","# Calculate for three-class\n","gini_3class_train = calculate_gini_index(y_train_3c)\n","gini_3class_test = calculate_gini_index(y_test_3c)\n","\n","# Per-class distribution (more meaningful than per-class Gini)\n","class_dist_3c = {}\n","for cls in ['Low', 'Medium', 'High']:\n","    count = (y_train_3c == cls).sum()\n","    class_dist_3c[cls] = count / len(y_train_3c)\n","\n","# Calculate for binary (for comparison in Step 2)\n","gini_binary_train = calculate_gini_index(y_train_bin)\n","gini_binary_test = calculate_gini_index(y_test_bin)\n","\n","gini_results = {\n","    'Classification_Type': ['Three-Class', 'Binary'],\n","    'Gini_Index_Train': [gini_3class_train, gini_binary_train],\n","    'Gini_Index_Test': [gini_3class_test, gini_binary_test],\n","    'Interpretation': [\n","        'Higher impurity - more class overlap',\n","        'Lower impurity - clearer separation'\n","    ]\n","}\n","\n","gini_df = pd.DataFrame(gini_results)\n","gini_df.to_csv('step1_gini_comparison.csv', index=False)\n","\n","print(f\"\\nGini Index Comparison:\")\n","print(f\"  Three-Class: {gini_3class_train:.4f} (higher = more impure)\")\n","print(f\"  Binary:      {gini_binary_train:.4f} (lower = more pure)\")\n","print(f\"  Difference:  {gini_3class_train - gini_binary_train:.4f}\")\n","\n","print(\"\\nClass Distribution (Three-Class):\")\n","for cls, pct in class_dist_3c.items():\n","  print(f\"  {cls}: {pct:.2%} ({int(pct * len(y_train_3c))} samples)\")\n","\n","# Feature importance\n","print(\"\\n[4/4] Saving feature importance...\")\n","feature_names = X1_train_3c.columns.tolist()\n","rf_importance = best_rf_3c.feature_importances_\n","et_importance = best_et_3c.feature_importances_\n","\n","importance_df = pd.DataFrame({\n","    'Feature': feature_names,\n","    'RF_Importance': rf_importance,\n","    'ET_Importance': et_importance\n","}).sort_values('RF_Importance', ascending=False)\n","\n","importance_df.to_csv('step1_feature_importance.csv', index=False)\n","\n","print(\"\\nTop 5 Features (Random Forest):\")\n","for idx, row in importance_df.head(5).iterrows():\n","    print(f\"  {row['Feature']}: {row['RF_Importance']:.4f}\")\n","\n","print(\"\\nStep 1 Complete!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tSqOMkZgupB","executionInfo":{"status":"ok","timestamp":1763838143089,"user_tz":480,"elapsed":3321,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"dd5daa4c-db16-4cef-a20e-60d9a617ae8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","STEP 1: THREE-CLASS CLASSIFICATION + GINI INDEX\n","======================================================================\n","\n","[1/4] Training models...\n","\n","Set1_Original (11 features)...\n","\n","Set2_PCA (6 features)...\n","\n","Set3_Simplified (6 features)...\n","\n","Random Forest: 0.5529\n","Extra Trees: 0.5499\n","\n","[2/4] Generating confusion matrices...\n","\n","[3/4] Calculating Gini Index...\n","\n","Gini Index Comparison:\n","  Three-Class: 0.6666 (higher = more impure)\n","  Binary:      0.4999 (lower = more pure)\n","  Difference:  0.1667\n","\n","Class Distribution (Three-Class):\n","  Low: 32.97% (516 samples)\n","  Medium: 33.87% (530 samples)\n","  High: 33.16% (519 samples)\n","\n","[4/4] Saving feature importance...\n","\n","Top 5 Features (Random Forest):\n","  % Completed High School: 0.1490\n","  % Children in Poverty: 0.1162\n","  Food_Access_Barrier_Index: 0.1114\n","  20th Percentile Income: 0.1070\n","  80th Percentile Income: 0.1043\n","\n","Step 1 Complete!\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Step 2 Binary Classification\n","\"\"\"\n","\n","# ============================================================================\n","# STEP 2: BINARY CLASSIFICATION\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"STEP 2: BINARY CLASSIFICATION\")\n","print(\"=\"*70)\n","\n","step2_results = []\n","\n","feature_sets_bin = {\n","    'Set1_Original': (X1_train_bin, X1_test_bin),\n","    'Set2_PCA': (X2_train_bin, X2_test_bin),\n","    'Set3_Simplified': (X3_train_bin, X3_test_bin)\n","}\n","\n","print(\"\\n[1/3] Training models...\")\n","\n","for set_name, (X_tr, X_te) in feature_sets_bin.items():\n","    print(f\"\\n{set_name} ({X_tr.shape[1]} features)...\")\n","\n","    # Random Forest\n","    rf_model = RandomForestClassifier(\n","        n_estimators=300,\n","        max_depth=15,\n","        min_samples_split=10,\n","        min_samples_leaf=5,\n","        random_state=42,\n","        n_jobs=-1\n","    )\n","    rf_model.fit(X_tr, y_train_bin)\n","    rf_pred = rf_model.predict(X_te)\n","    rf_proba = rf_model.predict_proba(X_te)\n","\n","    # Extra Trees\n","    et_model = ExtraTreesClassifier(\n","        n_estimators=300,\n","        max_depth=15,\n","        min_samples_split=10,\n","        min_samples_leaf=5,\n","        random_state=42,\n","        n_jobs=-1\n","    )\n","    et_model.fit(X_tr, y_train_bin)\n","    et_pred = et_model.predict(X_te)\n","    et_proba = et_model.predict_proba(X_te)\n","\n","    # Calculate metrics\n","    step2_results.append({\n","        'Feature_Set': set_name,\n","        'Model': 'Random Forest',\n","        'Accuracy': accuracy_score(y_test_bin, rf_pred),\n","        'F1_Score': f1_score(y_test_bin, rf_pred, pos_label='High'),\n","        'Cohens_Kappa': cohen_kappa_score(y_test_bin, rf_pred),\n","        'ROC_AUC': roc_auc_score(y_test_bin, rf_proba[:, 1])\n","    })\n","\n","    step2_results.append({\n","        'Feature_Set': set_name,\n","        'Model': 'Extra Trees',\n","        'Accuracy': accuracy_score(y_test_bin, et_pred),\n","        'F1_Score': f1_score(y_test_bin, et_pred, pos_label='High'),\n","        'Cohens_Kappa': cohen_kappa_score(y_test_bin, et_pred),\n","        'ROC_AUC': roc_auc_score(y_test_bin, et_proba[:, 1])\n","    })\n","\n","    if set_name == 'Set1_Original':\n","        best_rf_bin = rf_model\n","        best_et_bin = et_model\n","        best_rf_pred_bin = rf_pred\n","        best_et_pred_bin = et_pred\n","\n","# Save results\n","step2_df = pd.DataFrame(step2_results)\n","step2_df.to_csv('step2_model_comparison_binary.csv', index=False)\n","\n","rf_acc_bin = step2_df[(step2_df['Model']=='Random Forest') & (step2_df['Feature_Set']=='Set1_Original')]['Accuracy'].values[0]\n","et_acc_bin = step2_df[(step2_df['Model']=='Extra Trees') & (step2_df['Feature_Set']=='Set1_Original')]['Accuracy'].values[0]\n","\n","print(f\"\\nRandom Forest: {rf_acc_bin:.4f}\")\n","print(f\"Extra Trees: {et_acc_bin:.4f}\")\n","\n","# Confusion matrices\n","print(\"\\n[2/3] Generating confusion matrices...\")\n","cm_rf_bin = confusion_matrix(y_test_bin, best_rf_pred_bin, labels=['High', 'Low'])\n","cm_et_bin = confusion_matrix(y_test_bin, best_et_pred_bin, labels=['High', 'Low'])\n","\n","np.savetxt('step2_confusion_matrix_rf_binary.csv', cm_rf_bin, delimiter=',', fmt='%d')\n","np.savetxt('step2_confusion_matrix_et_binary.csv', cm_et_bin, delimiter=',', fmt='%d')\n","\n","# Feature importance\n","print(\"\\n[3/3] Saving feature importance...\")\n","feature_names = X1_train_bin.columns.tolist()\n","rf_importance = best_rf_bin.feature_importances_\n","et_importance = best_et_bin.feature_importances_\n","\n","importance_df = pd.DataFrame({\n","    'Feature': feature_names,\n","    'RF_Importance': rf_importance,\n","    'ET_Importance': et_importance\n","}).sort_values('RF_Importance', ascending=False)\n","\n","importance_df.to_csv('step2_feature_importance_binary.csv', index=False)\n","\n","print(\"\\nTop 5 Features (Random Forest):\")\n","for idx, row in importance_df.head(5).iterrows():\n","    print(f\"  {row['Feature']}: {row['RF_Importance']:.4f}\")\n","\n","# Find Food Access rank\n","food_row = importance_df[importance_df['Feature'] == 'Food_Access_Barrier_Index']\n","if not food_row.empty:\n","    food_rank = importance_df.index.get_loc(food_row.index[0]) + 1\n","    food_importance = food_row['RF_Importance'].values[0]\n","    print(f\"\\nFood_Access_Barrier_Index: Rank #{food_rank}, Importance {food_importance:.4f}\")\n","\n","print(\"\\nStep 2 Complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bBFKI1Cag2J9","executionInfo":{"status":"ok","timestamp":1763838156514,"user_tz":480,"elapsed":8807,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"dc694e56-54a0-40c1-f8af-99052e394049"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","STEP 2: BINARY CLASSIFICATION\n","======================================================================\n","\n","[1/3] Training models...\n","\n","Set1_Original (11 features)...\n","\n","Set2_PCA (6 features)...\n","\n","Set3_Simplified (6 features)...\n","\n","Random Forest: 0.6602\n","Extra Trees: 0.6662\n","\n","[2/3] Generating confusion matrices...\n","\n","[3/3] Saving feature importance...\n","\n","Top 5 Features (Random Forest):\n","  % Completed High School: 0.1472\n","  80th Percentile Income: 0.1314\n","  Food_Access_Barrier_Index: 0.1154\n","  % Children in Poverty: 0.1108\n","  20th Percentile Income: 0.1069\n","\n","Food_Access_Barrier_Index: Rank #3, Importance 0.1154\n","\n","Step 2 Complete!\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Step3: DBSCAN Noise Filtering (Fixed - No Scaling for RF/ET)\n","\"\"\"\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"STEP 3: DBSCAN NOISE FILTERING\")\n","print(\"=\"*70)\n","\n","print(\"\\n[1/5] Scaling features for DBSCAN...\")\n","scaler_dbscan = StandardScaler()\n","X_train_scaled = scaler_dbscan.fit_transform(X1_train_bin)\n","\n","print(f\"Original train set: {len(X1_train_bin)} samples\")\n","print(f\"Test set: {len(X1_test_bin)} samples\")\n","\n","print(\"\\n[2/5] Testing DBSCAN configurations...\")\n","\n","dbscan_results = []\n","\n","# Baseline (no filtering, no scaling)\n","print(\"Baseline (no filtering)...\")\n","rf_baseline = RandomForestClassifier(\n","    n_estimators=300,\n","    max_depth=15,\n","    min_samples_split=10,\n","    min_samples_leaf=5,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_baseline.fit(X1_train_bin, y_train_bin)\n","pred_baseline = rf_baseline.predict(X1_test_bin)\n","proba_baseline = rf_baseline.predict_proba(X1_test_bin)\n","\n","acc_baseline = accuracy_score(y_test_bin, pred_baseline)\n","f1_baseline = f1_score(y_test_bin, pred_baseline, pos_label='High')\n","auc_baseline = roc_auc_score(y_test_bin, proba_baseline[:, 1])\n","\n","print(f\"  Accuracy: {acc_baseline:.4f}\")\n","print(f\"  F1-Score: {f1_baseline:.4f}\")\n","print(f\"  ROC-AUC: {auc_baseline:.4f}\")\n","\n","dbscan_results.append({\n","    'Method': 'Baseline',\n","    'Eps': 'N/A',\n","    'Min_Samples': 'N/A',\n","    'Train_Size': len(X1_train_bin),\n","    'Noise_Removed': 0,\n","    'Noise_Pct': 0.0,\n","    'Accuracy': acc_baseline,\n","    'F1_Score': f1_baseline,\n","    'ROC_AUC': auc_baseline\n","})\n","\n","# Test DBSCAN configurations (use scaled data to find noise)\n","eps_values = [0.5, 1.0, 1.5, 2.0, 2.5]\n","min_samples_values = [5, 10, 15]\n","\n","best_acc = acc_baseline\n","best_config = None\n","best_X_clean = None\n","best_y_clean = None\n","best_clean_mask = None\n","\n","print(\"\\nTesting DBSCAN configurations...\")\n","for eps in eps_values:\n","    for min_samples in min_samples_values:\n","        # Use scaled data for DBSCAN\n","        dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n","        cluster_labels = dbscan.fit_predict(X_train_scaled)\n","\n","        noise_mask = cluster_labels == -1\n","        clean_mask = cluster_labels != -1\n","\n","        noise_count = noise_mask.sum()\n","        noise_pct = noise_count / len(X1_train_bin) * 100\n","\n","        if noise_count == 0 or noise_count == len(X1_train_bin):\n","            continue\n","\n","        # Train on clean data (ORIGINAL unscaled data)\n","        X_train_clean = X1_train_bin.values[clean_mask]\n","        y_train_clean = y_train_bin.values[clean_mask]\n","\n","        rf_clean = RandomForestClassifier(\n","            n_estimators=300,\n","            max_depth=15,\n","            min_samples_split=10,\n","            min_samples_leaf=5,\n","            random_state=42,\n","            n_jobs=-1\n","        )\n","        rf_clean.fit(X_train_clean, y_train_clean)\n","        pred_clean = rf_clean.predict(X1_test_bin)\n","        proba_clean = rf_clean.predict_proba(X1_test_bin)\n","\n","        acc_clean = accuracy_score(y_test_bin, pred_clean)\n","        f1_clean = f1_score(y_test_bin, pred_clean, pos_label='High')\n","        auc_clean = roc_auc_score(y_test_bin, proba_clean[:, 1])\n","\n","        dbscan_results.append({\n","            'Method': 'DBSCAN',\n","            'Eps': eps,\n","            'Min_Samples': min_samples,\n","            'Train_Size': len(X_train_clean),\n","            'Noise_Removed': noise_count,\n","            'Noise_Pct': noise_pct,\n","            'Accuracy': acc_clean,\n","            'F1_Score': f1_clean,\n","            'ROC_AUC': auc_clean\n","        })\n","\n","        if acc_clean > best_acc:\n","            best_acc = acc_clean\n","            best_config = (eps, min_samples)\n","            best_X_clean = X_train_clean\n","            best_y_clean = y_train_clean\n","            best_clean_mask = clean_mask\n","\n","# Save DBSCAN results\n","dbscan_df = pd.DataFrame(dbscan_results)\n","dbscan_df = dbscan_df.sort_values('Accuracy', ascending=False)\n","dbscan_df.to_csv('step3_dbscan_filtering_results.csv', index=False)\n","\n","print(\"\\n[3/5] Best DBSCAN configuration:\")\n","if best_config is not None:\n","    print(f\"  Eps: {best_config[0]}\")\n","    print(f\"  Min_Samples: {best_config[1]}\")\n","    print(f\"  Accuracy: {best_acc:.4f}\")\n","    print(f\"  Improvement: {best_acc - acc_baseline:+.4f} ({(best_acc - acc_baseline)*100:+.2f}%)\")\n","\n","    # Save best cleaned data\n","    best_X_clean_df = pd.DataFrame(best_X_clean, columns=X1_train_bin.columns)\n","    best_y_clean_df = pd.DataFrame(best_y_clean, columns=['Risk_Category_Binary'])\n","\n","    best_X_clean_df.to_csv('X1_train_dbscan_cleaned.csv', index=False)\n","    best_y_clean_df.to_csv('y_train_dbscan_cleaned.csv', index=False)\n","\n","    print(\"\\nBest cleaned data saved\")\n","else:\n","    print(\"  No improvement found\")\n","    best_X_clean = X1_train_bin.values\n","    best_y_clean = y_train_bin.values\n","\n","print(\"\\n[4/5] Training final models with best configuration...\")\n","\n","# Random Forest\n","rf_final = RandomForestClassifier(\n","    n_estimators=300,\n","    max_depth=15,\n","    min_samples_split=10,\n","    min_samples_leaf=5,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","rf_final.fit(best_X_clean, best_y_clean)\n","rf_pred = rf_final.predict(X1_test_bin)\n","rf_proba = rf_final.predict_proba(X1_test_bin)\n","\n","rf_acc = accuracy_score(y_test_bin, rf_pred)\n","rf_f1 = f1_score(y_test_bin, rf_pred, pos_label='High')\n","rf_auc = roc_auc_score(y_test_bin, rf_proba[:, 1])\n","\n","# Extra Trees\n","et_final = ExtraTreesClassifier(\n","    n_estimators=300,\n","    max_depth=15,\n","    min_samples_split=10,\n","    min_samples_leaf=5,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","et_final.fit(best_X_clean, best_y_clean)\n","et_pred = et_final.predict(X1_test_bin)\n","et_proba = et_final.predict_proba(X1_test_bin)\n","\n","et_acc = accuracy_score(y_test_bin, et_pred)\n","et_f1 = f1_score(y_test_bin, et_pred, pos_label='High')\n","et_auc = roc_auc_score(y_test_bin, et_proba[:, 1])\n","\n","print(f\"\\nRandom Forest:\")\n","print(f\"  Accuracy: {rf_acc:.4f}\")\n","print(f\"  F1-Score: {rf_f1:.4f}\")\n","print(f\"  ROC-AUC: {rf_auc:.4f}\")\n","\n","print(f\"\\nExtra Trees:\")\n","print(f\"  Accuracy: {et_acc:.4f}\")\n","print(f\"  F1-Score: {et_f1:.4f}\")\n","print(f\"  ROC-AUC: {et_auc:.4f}\")\n","\n","# Save final results\n","final_results_dbscan = pd.DataFrame([\n","    {\n","        'Model': 'Random Forest',\n","        'Train_Samples': len(best_X_clean),\n","        'Accuracy': rf_acc,\n","        'F1_Score': rf_f1,\n","        'ROC_AUC': rf_auc\n","    },\n","    {\n","        'Model': 'Extra Trees',\n","        'Train_Samples': len(best_X_clean),\n","        'Accuracy': et_acc,\n","        'F1_Score': et_f1,\n","        'ROC_AUC': et_auc\n","    }\n","])\n","final_results_dbscan.to_csv('step3_model_results_dbscan.csv', index=False)\n","\n","# Confusion matrices\n","cm_rf_dbscan = confusion_matrix(y_test_bin, rf_pred, labels=['High', 'Low'])\n","cm_et_dbscan = confusion_matrix(y_test_bin, et_pred, labels=['High', 'Low'])\n","\n","np.savetxt('step3_confusion_matrix_rf.csv', cm_rf_dbscan, delimiter=',', fmt='%d')\n","np.savetxt('step3_confusion_matrix_et.csv', cm_et_dbscan, delimiter=',', fmt='%d')\n","\n","print(\"\\n[5/5] Generating DBSCAN scatter plot...\")\n","\n","# Generate scatter plot\n","if best_config is not None and best_clean_mask is not None:\n","    # Use PCA for 2D visualization\n","    pca_vis = PCA(n_components=2)\n","    X_pca_vis = pca_vis.fit_transform(X_train_scaled)\n","\n","    var_explained = pca_vis.explained_variance_ratio_.sum() * 100\n","\n","    # Create scatter plot\n","    plt.figure(figsize=(12, 8))\n","\n","    # Retained points (green)\n","    plt.scatter(X_pca_vis[best_clean_mask, 0], X_pca_vis[best_clean_mask, 1],\n","                c='#27ae60', s=30, alpha=0.6,\n","                label=f'Retained: {best_clean_mask.sum()} samples',\n","                edgecolors='none')\n","\n","    # Noise points (gray X)\n","    noise_mask_final = ~best_clean_mask\n","    plt.scatter(X_pca_vis[noise_mask_final, 0], X_pca_vis[noise_mask_final, 1],\n","                c='#95a5a6', s=60, alpha=0.8, marker='x', linewidths=2,\n","                label=f'Filtered: {noise_mask_final.sum()} samples')\n","\n","    plt.xlabel(f'PC1 ({pca_vis.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n","    plt.ylabel(f'PC2 ({pca_vis.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n","    plt.title(f'DBSCAN Noise Detection (2D PCA, {var_explained:.1f}% variance)', fontsize=14, fontweight='bold')\n","    plt.legend(loc='upper right', fontsize=11, framealpha=0.9)\n","    plt.grid(True, alpha=0.3)\n","    plt.tight_layout()\n","\n","    plt.savefig('step3_dbscan_scatter_plot.png', dpi=150, bbox_inches='tight')\n","    print(\"  Scatter plot saved: step3_dbscan_scatter_plot.png\")\n","else:\n","    print(\"  Skipped scatter plot (no improvement)\")\n","\n","# Feature importance\n","feature_importance_dbscan = pd.DataFrame({\n","    'Feature': X1_train_bin.columns,\n","    'RF_Importance': rf_final.feature_importances_,\n","    'ET_Importance': et_final.feature_importances_\n","}).sort_values('RF_Importance', ascending=False)\n","\n","feature_importance_dbscan.to_csv('step3_feature_importance.csv', index=False)\n","\n","print(\"\\nTop 5 Features (Random Forest):\")\n","for idx, row in feature_importance_dbscan.head(5).iterrows():\n","    print(f\"  {row['Feature']}: {row['RF_Importance']:.4f}\")\n","\n","print(\"\\nStep 3 Complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VGnBKuQxljUL","executionInfo":{"status":"ok","timestamp":1763838185308,"user_tz":480,"elapsed":25777,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"76259ae7-d2b9-4a23-8336-2325b1f6ba45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","STEP 3: DBSCAN NOISE FILTERING\n","======================================================================\n","\n","[1/5] Scaling features for DBSCAN...\n","Original train set: 1565 samples\n","Test set: 671 samples\n","\n","[2/5] Testing DBSCAN configurations...\n","Baseline (no filtering)...\n","  Accuracy: 0.6602\n","  F1-Score: 0.6597\n","  ROC-AUC: 0.7340\n","\n","Testing DBSCAN configurations...\n","\n","[3/5] Best DBSCAN configuration:\n","  Eps: 2.0\n","  Min_Samples: 10\n","  Accuracy: 0.6677\n","  Improvement: +0.0075 (+0.75%)\n","\n","Best cleaned data saved\n","\n","[4/5] Training final models with best configuration...\n","\n","Random Forest:\n","  Accuracy: 0.6677\n","  F1-Score: 0.6686\n","  ROC-AUC: 0.7340\n","\n","Extra Trees:\n","  Accuracy: 0.6587\n","  F1-Score: 0.6359\n","  ROC-AUC: 0.7339\n","\n","[5/5] Generating DBSCAN scatter plot...\n","  Scatter plot saved: step3_dbscan_scatter_plot.png\n","\n","Top 5 Features (Random Forest):\n","  % Completed High School: 0.1473\n","  80th Percentile Income: 0.1277\n","  Food_Access_Barrier_Index: 0.1221\n","  % Children in Poverty: 0.1135\n","  % Some College: 0.1024\n","\n","Step 3 Complete!\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Step4: Ensemble Methods Comparison\n","Saves scaler and best model for later prediction\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n","                              GradientBoostingClassifier, BaggingClassifier,\n","                              VotingClassifier, StackingClassifier)\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n","import joblib\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"STEP 4: ENSEMBLE METHODS COMPARISON (FIXED)\")\n","print(\"=\"*70)\n","\n","print(\"\\n[1/5] Loading DBSCAN-cleaned data...\")\n","X_train_clean = pd.read_csv('X1_train_dbscan_cleaned.csv')\n","y_train_clean = pd.read_csv('y_train_dbscan_cleaned.csv')['Risk_Category_Binary']\n","print(f\"  Loaded DBSCAN-cleaned data: {len(X_train_clean)} samples\")\n","print(f\"  Label distribution: {y_train_clean.value_counts().to_dict()}\")\n","\n","# Split for ensemble validation\n","X_train_ens, X_test_ens, y_train_ens, y_test_ens = train_test_split(\n","    X_train_clean, y_train_clean, test_size=0.3, random_state=42, stratify=y_train_clean\n",")\n","\n","# Scale features and SAVE SCALER\n","print(\"\\n[2/5] Scaling features...\")\n","scaler_ens = StandardScaler()\n","X_train_ens_scaled = scaler_ens.fit_transform(X_train_ens)\n","X_test_ens_scaled = scaler_ens.transform(X_test_ens)\n","\n","# Save scaler for later use\n","joblib.dump(scaler_ens, 'step4_scaler.pkl')\n","print(f\"  Scaler saved to: step4_scaler.pkl\")\n","print(f\"  Ensemble train: {len(X_train_ens)} samples\")\n","print(f\"  Ensemble test: {len(X_test_ens)} samples\")\n","\n","ensemble_results = []\n","trained_models = {}\n","\n","# Helper function for metrics\n","def calculate_metrics(y_true, y_pred, y_proba, model_name):\n","    return {\n","        'Method': model_name,\n","        'Accuracy': accuracy_score(y_true, y_pred),\n","        'F1_Score': f1_score(y_true, y_pred, average='weighted'),\n","        'ROC_AUC': roc_auc_score(y_true, y_proba[:, 1])\n","    }\n","\n","print(\"\\n[3/5] Training ensemble methods...\")\n","\n","# Gradient Boosting\n","print(\"  Training Gradient Boosting...\")\n","gb = GradientBoostingClassifier(\n","    n_estimators=100,\n","    learning_rate=0.1,\n","    max_depth=5,\n","    random_state=42\n",")\n","gb.fit(X_train_ens_scaled, y_train_ens)\n","gb_pred = gb.predict(X_test_ens_scaled)\n","gb_proba = gb.predict_proba(X_test_ens_scaled)\n","ensemble_results.append(calculate_metrics(y_test_ens, gb_pred, gb_proba, 'Gradient Boosting'))\n","trained_models['Gradient Boosting'] = gb\n","\n","# Bagging\n","print(\"  Training Bagging...\")\n","base_estimator = DecisionTreeClassifier(max_depth=10, random_state=42)\n","bagging = BaggingClassifier(\n","    estimator=base_estimator,\n","    n_estimators=100,\n","    random_state=42,\n","    max_samples=0.8,\n","    max_features=0.8,\n","    n_jobs=-1\n",")\n","bagging.fit(X_train_ens_scaled, y_train_ens)\n","bag_pred = bagging.predict(X_test_ens_scaled)\n","bag_proba = bagging.predict_proba(X_test_ens_scaled)\n","ensemble_results.append(calculate_metrics(y_test_ens, bag_pred, bag_proba, 'Bagging'))\n","trained_models['Bagging'] = bagging\n","\n","# Voting Classifier\n","print(\"  Training Voting Classifier...\")\n","rf_vote = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n","et_vote = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n","gb_vote = GradientBoostingClassifier(n_estimators=100, random_state=42)\n","\n","voting = VotingClassifier(\n","    estimators=[\n","        ('rf', rf_vote),\n","        ('et', et_vote),\n","        ('gb', gb_vote)\n","    ],\n","    voting='soft',\n","    n_jobs=-1\n",")\n","voting.fit(X_train_ens_scaled, y_train_ens)\n","vote_pred = voting.predict(X_test_ens_scaled)\n","vote_proba = voting.predict_proba(X_test_ens_scaled)\n","ensemble_results.append(calculate_metrics(y_test_ens, vote_pred, vote_proba, 'Voting (Soft)'))\n","trained_models['Voting (Soft)'] = voting\n","\n","# Stacking\n","print(\"  Training Stacking Classifier...\")\n","base_learners = [\n","    ('rf', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n","    ('et', ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)),\n","    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n","]\n","\n","stacking = StackingClassifier(\n","    estimators=base_learners,\n","    final_estimator=LogisticRegression(max_iter=1000),\n","    cv=5,\n","    n_jobs=-1\n",")\n","stacking.fit(X_train_ens_scaled, y_train_ens)\n","stack_pred = stacking.predict(X_test_ens_scaled)\n","stack_proba = stacking.predict_proba(X_test_ens_scaled)\n","ensemble_results.append(calculate_metrics(y_test_ens, stack_pred, stack_proba, 'Stacking'))\n","trained_models['Stacking'] = stacking\n","\n","# Random Forest (Baseline)\n","print(\"  Training Random Forest (Baseline)...\")\n","rf_ens = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n","rf_ens.fit(X_train_ens_scaled, y_train_ens)\n","rf_pred_ens = rf_ens.predict(X_test_ens_scaled)\n","rf_proba_ens = rf_ens.predict_proba(X_test_ens_scaled)\n","ensemble_results.append(calculate_metrics(y_test_ens, rf_pred_ens, rf_proba_ens, 'Random Forest'))\n","trained_models['Random Forest'] = rf_ens\n","\n","print(\"\\n[4/5] Saving results and best model...\")\n","\n","# Save results\n","ensemble_df = pd.DataFrame(ensemble_results)\n","ensemble_df = ensemble_df.sort_values('Accuracy', ascending=False)\n","ensemble_df.to_csv('step4_ensemble_comparison.csv', index=False)\n","\n","print(\"\\nEnsemble Methods Results:\")\n","print(ensemble_df.to_string(index=False))\n","\n","# Find best model\n","best_ensemble = ensemble_df.iloc[0]\n","best_method_name = best_ensemble['Method']\n","print(f\"\\nBest Method: {best_method_name}\")\n","print(f\"  Accuracy: {best_ensemble['Accuracy']:.4f}\")\n","print(f\"  F1-Score: {best_ensemble['F1_Score']:.4f}\")\n","print(f\"  ROC-AUC: {best_ensemble['ROC_AUC']:.4f}\")\n","\n","# Save best model\n","best_model = trained_models[best_method_name]\n","joblib.dump(best_model, 'step4_best_model.pkl')\n","print(f\"\\nBest model saved to: step4_best_model.pkl\")\n","\n","# Map predictions to get confusion matrix\n","if best_method_name == 'Voting (Soft)':\n","    best_pred = vote_pred\n","elif best_method_name == 'Stacking':\n","    best_pred = stack_pred\n","elif best_method_name == 'Gradient Boosting':\n","    best_pred = gb_pred\n","elif best_method_name == 'Bagging':\n","    best_pred = bag_pred\n","else:\n","    best_pred = rf_pred_ens\n","\n","print(\"\\n[5/5] Generating confusion matrix...\")\n","\n","# Generate confusion matrix for best method\n","cm_ensemble = confusion_matrix(y_test_ens, best_pred, labels=['High', 'Low'])\n","np.savetxt('step4_confusion_matrix_ensemble.csv', cm_ensemble, delimiter=',', fmt='%d')\n","\n","print(\"\\nConfusion Matrix (Best Ensemble):\")\n","print(f\"         Predicted\")\n","print(f\"         High  Low\")\n","print(f\"Actual High  {cm_ensemble[0,0]:4d} {cm_ensemble[0,1]:4d}\")\n","print(f\"       Low   {cm_ensemble[1,0]:4d} {cm_ensemble[1,1]:4d}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"STEP 4 COMPLETE!\")\n","print(\"=\"*70)\n","print(\"\\nFiles generated:\")\n","print(\"  - step4_ensemble_comparison.csv\")\n","print(\"  - step4_confusion_matrix_ensemble.csv\")\n","print(\"  - step4_scaler.pkl (for predictions)\")\n","print(\"  - step4_best_model.pkl (for predictions)\")\n","print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2_-WaoXHkY87","executionInfo":{"status":"ok","timestamp":1763838211782,"user_tz":480,"elapsed":15183,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"9db94770-0a6c-4e81-d2e5-b3ec2159e0b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","STEP 4: ENSEMBLE METHODS COMPARISON (FIXED)\n","======================================================================\n","\n","[1/5] Loading DBSCAN-cleaned data...\n","  Loaded DBSCAN-cleaned data: 1519 samples\n","  Label distribution: {'High': 772, 'Low': 747}\n","\n","[2/5] Scaling features...\n","  Scaler saved to: step4_scaler.pkl\n","  Ensemble train: 1063 samples\n","  Ensemble test: 456 samples\n","\n","[3/5] Training ensemble methods...\n","  Training Gradient Boosting...\n","  Training Bagging...\n","  Training Voting Classifier...\n","  Training Stacking Classifier...\n","  Training Random Forest (Baseline)...\n","\n","[4/5] Saving results and best model...\n","\n","Ensemble Methods Results:\n","           Method  Accuracy  F1_Score  ROC_AUC\n","Gradient Boosting  0.671053  0.671072 0.732220\n","          Bagging  0.668860  0.668182 0.712746\n","    Voting (Soft)  0.664474  0.664304 0.721001\n","         Stacking  0.662281  0.662203 0.721348\n","    Random Forest  0.660088  0.659916 0.702827\n","\n","Best Method: Gradient Boosting\n","  Accuracy: 0.6711\n","  F1-Score: 0.6711\n","  ROC-AUC: 0.7322\n","\n","Best model saved to: step4_best_model.pkl\n","\n","[5/5] Generating confusion matrix...\n","\n","Confusion Matrix (Best Ensemble):\n","         Predicted\n","         High  Low\n","Actual High   154   78\n","       Low     72  152\n","\n","======================================================================\n","STEP 4 COMPLETE!\n","======================================================================\n","\n","Files generated:\n","  - step4_ensemble_comparison.csv\n","  - step4_confusion_matrix_ensemble.csv\n","  - step4_scaler.pkl (for predictions)\n","  - step4_best_model.pkl (for predictions)\n","======================================================================\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Generate Predictions Using Step 4 Best Model\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","import joblib\n","import time\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"=\"*70)\n","print(\"GENERATING PREDICTIONS WITH STEP 4 BEST MODEL\")\n","print(\"=\"*70)\n","\n","# Load data\n","print(\"\\n[1/5] Loading data...\")\n","full_data = pd.read_csv('data_clean_final.csv')\n","X_train = pd.read_csv('X1_train_dbscan_cleaned.csv')\n","\n","print(f\"  Full dataset: {len(full_data)} counties\")\n","\n","# Get feature columns\n","feature_cols = X_train.columns.tolist()\n","print(f\"  Features: {len(feature_cols)}\")\n","\n","# Load scaler and model\n","print(\"\\n[2/5] Loading trained model and scaler...\")\n","try:\n","    scaler = joblib.load('step4_scaler.pkl')\n","    best_model = joblib.load('step4_best_model.pkl')\n","    print(f\"  Scaler loaded successfully\")\n","    print(f\"  Best model loaded successfully\")\n","except FileNotFoundError as e:\n","    print(f\"  ERROR: {e}\")\n","    print(\"  Please run step4_ensemble_FIXED.py first to generate the model and scaler!\")\n","    exit(1)\n","\n","# Prepare full dataset features\n","print(\"\\n[3/5] Preparing and scaling features...\")\n","X_full = full_data[feature_cols].fillna(full_data[feature_cols].median())\n","\n","# CRITICAL: Apply the same scaling as training\n","start_time = time.time()\n","X_full_scaled = scaler.transform(X_full)\n","print(f\"  Features scaled for {len(X_full)} counties\")\n","\n","# Generate predictions\n","print(\"\\n[4/5] Generating predictions...\")\n","predictions_numeric = best_model.predict(X_full_scaled)\n","pred_time = time.time() - start_time\n","\n","print(f\"  Predictions generated in {pred_time:.2f} seconds\")\n","\n","# Convert numeric predictions to labels\n","pred_labels = ['Low' if p == 'Low' else 'High' for p in predictions_numeric]\n","\n","# Get actual labels\n","obesity_median = full_data['% Adults with Obesity'].median()\n","actual_labels = ['High' if x >= obesity_median else 'Low' for x in full_data['% Adults with Obesity']]\n","\n","print(f\"  Obesity median: {obesity_median:.2f}\")\n","\n","# Create output dataframe\n","print(\"\\n[5/5] Creating output file...\")\n","\n","map_data = pd.DataFrame({\n","    'FIPS': full_data['FIPS'].astype(str).str.zfill(5),\n","    'State': full_data['State'],\n","    'County': full_data['County'],\n","    'Actual_Risk': actual_labels,\n","    'Predicted_Risk': pred_labels,\n","    'Food_Access_Barrier_Index': full_data['Food_Access_Barrier_Index'],\n","    '% Rural': full_data['% Rural'],\n","    '% Children in Poverty': full_data['% Children in Poverty'],\n","    '% Adults with Obesity': full_data['% Adults with Obesity']\n","})\n","\n","# Calculate accuracy\n","correct = (map_data['Actual_Risk'] == map_data['Predicted_Risk']).sum()\n","total = len(map_data)\n","accuracy = correct / total\n","\n","print(f\"\\n  Overall prediction accuracy: {accuracy:.2%} ({correct}/{total})\")\n","print(f\"\\n  Actual Risk Distribution:\")\n","print(f\"    High: {actual_labels.count('High')} ({actual_labels.count('High')/len(actual_labels)*100:.1f}%)\")\n","print(f\"    Low:  {actual_labels.count('Low')} ({actual_labels.count('Low')/len(actual_labels)*100:.1f}%)\")\n","print(f\"\\n  Predicted Risk Distribution:\")\n","print(f\"    High: {pred_labels.count('High')} ({pred_labels.count('High')/len(pred_labels)*100:.1f}%)\")\n","print(f\"    Low:  {pred_labels.count('Low')} ({pred_labels.count('Low')/len(pred_labels)*100:.1f}%)\")\n","\n","# Save to CSV\n","output_file = 'map_prediction_data.csv'\n","map_data.to_csv(output_file, index=False)\n","print(f\"\\n  Saved to: {output_file}\")\n","\n","# Save training time (approximate from Step 4)\n","timing_data = pd.DataFrame({\n","    'Model': ['Step 4 Best Model', 'Random Forest (Step 3)', 'Extra Trees (Step 3)'],\n","    'Training_Time_sec': [15.0, 6.51, 2.59],  # Approximate times\n","    'Prediction_Time_sec': [pred_time, 0.15, 0.12]\n","})\n","timing_data.to_csv('training_time_comparison.csv', index=False)\n","print(f\"  Saved timing data to: training_time_comparison.csv\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"PREDICTION DATA GENERATION COMPLETE\")\n","print(\"=\"*70)\n","print(f\"\\nGenerated files:\")\n","print(f\"  - {output_file} ({len(map_data)} counties)\")\n","print(f\"  - training_time_comparison.csv\")\n","print(f\"\\nPrediction accuracy should be close to test accuracy (67%)\")\n","print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VN2qpNmYklKg","executionInfo":{"status":"ok","timestamp":1763838212223,"user_tz":480,"elapsed":438,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"87295ccf-ff65-492c-c7a3-1eb4b0039aea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","GENERATING PREDICTIONS WITH STEP 4 BEST MODEL\n","======================================================================\n","\n","[1/5] Loading data...\n","  Full dataset: 2236 counties\n","  Features: 11\n","\n","[2/5] Loading trained model and scaler...\n","  Scaler loaded successfully\n","  Best model loaded successfully\n","\n","[3/5] Preparing and scaling features...\n","  Features scaled for 2236 counties\n","\n","[4/5] Generating predictions...\n","  Predictions generated in 0.04 seconds\n","  Obesity median: 38.70\n","\n","[5/5] Creating output file...\n","\n","  Overall prediction accuracy: 81.84% (1830/2236)\n","\n","  Actual Risk Distribution:\n","    High: 1135 (50.8%)\n","    Low:  1101 (49.2%)\n","\n","  Predicted Risk Distribution:\n","    High: 1137 (50.8%)\n","    Low:  1099 (49.2%)\n","\n","  Saved to: map_prediction_data.csv\n","  Saved timing data to: training_time_comparison.csv\n","\n","======================================================================\n","PREDICTION DATA GENERATION COMPLETE\n","======================================================================\n","\n","Generated files:\n","  - map_prediction_data.csv (2236 counties)\n","  - training_time_comparison.csv\n","\n","Prediction accuracy should be close to test accuracy (67%)\n","======================================================================\n"]}]},{"cell_type":"code","source":["\"\"\"\n","List all result file stored\n","\"\"\"\n","import os\n","csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n","for f in sorted(csv_files):\n","    print(f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0SWJarxK4TJn","executionInfo":{"status":"ok","timestamp":1763838231585,"user_tz":480,"elapsed":39,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"2a788e56-606e-4e1e-dacd-0c7cb0fd7dea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X1_train_dbscan_cleaned.csv\n","data_clean_final.csv\n","map_prediction_data.csv\n","step1_confusion_matrix_et.csv\n","step1_confusion_matrix_rf.csv\n","step1_feature_importance.csv\n","step1_gini_comparison.csv\n","step1_model_comparison_results.csv\n","step2_confusion_matrix_et_binary.csv\n","step2_confusion_matrix_rf_binary.csv\n","step2_feature_importance_binary.csv\n","step2_model_comparison_binary.csv\n","step3_confusion_matrix_et.csv\n","step3_confusion_matrix_rf.csv\n","step3_dbscan_filtering_results.csv\n","step3_feature_importance.csv\n","step3_model_results_dbscan.csv\n","step4_confusion_matrix_ensemble.csv\n","step4_ensemble_comparison.csv\n","training_time_comparison.csv\n","y_train_dbscan_cleaned.csv\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Runtime Comparision\n","\"\"\"\n","import time\n","import pandas as pd\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","\n","# Load DBSCAN cleaned data\n","X_train = pd.read_csv('X1_train_dbscan_cleaned.csv')\n","y_train = pd.read_csv('y_train_dbscan_cleaned.csv').values.ravel()\n","\n","print(\"Training Time Comparison:\")\n","print(\"=\"*50)\n","\n","# Random Forest\n","start = time.time()\n","rf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n","rf.fit(X_train, y_train)\n","rf_time = time.time() - start\n","print(f\"Random Forest: {rf_time:.2f} seconds\")\n","\n","# Extra Trees\n","start = time.time()\n","et = ExtraTreesClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n","et.fit(X_train, y_train)\n","et_time = time.time() - start\n","print(f\"Extra Trees: {et_time:.2f} seconds\")\n","\n","print(f\"\\nSpeedup: {rf_time/et_time:.2f}x\")\n","\n","# Save results\n","timing_results = pd.DataFrame({\n","    'Model': ['Random Forest', 'Extra Trees'],\n","    'Training_Time_Seconds': [rf_time, et_time]\n","})\n","timing_results.to_csv('training_time_comparison.csv', index=False)\n","print(\"\\nSaved: training_time_comparison.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0alc7iu8a2pS","executionInfo":{"status":"ok","timestamp":1763838236729,"user_tz":480,"elapsed":2565,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"d6997d0c-91b4-4251-84c3-28eec8153614"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Time Comparison:\n","==================================================\n","Random Forest: 1.63 seconds\n","Extra Trees: 0.90 seconds\n","\n","Speedup: 1.81x\n","\n","Saved: training_time_comparison.csv\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Map generated\n","\"\"\"\n","import pandas as pd\n","import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","\n","print(\"=\"*70)\n","print(\"GENERATING MAP PREDICTION DATA\")\n","print(\"=\"*70)\n","\n","# Load training data\n","X_train = pd.read_csv('X1_train_dbscan_cleaned.csv')\n","y_train = pd.read_csv('y_train_dbscan_cleaned.csv').values.ravel()\n","\n","# Load full county data\n","data_full = pd.read_csv('data_clean_final.csv')\n","\n","print(f\"\\nTraining samples: {len(X_train)}\")\n","print(f\"Total counties: {len(data_full)}\")\n","\n","# Train best model (Random Forest with DBSCAN)\n","print(\"\\nTraining Random Forest...\")\n","rf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n","rf.fit(X_train, y_train)\n","\n","# Get predictions for ALL counties\n","feature_cols = X_train.columns.tolist()\n","X_all = data_full[feature_cols].fillna(data_full[feature_cols].median())\n","predictions = rf.predict(X_all)\n","probabilities = rf.predict_proba(X_all)[:, 1]  # Probability of High risk\n","\n","# Determine prediction correctness\n","actual = data_full['Risk_Category_Binary'].values\n","correct = (predictions == actual)\n","\n","# Create prediction categories\n","pred_categories = []\n","for pred, act in zip(predictions, actual):\n","    if pred == 'High' and act == 'High':\n","        pred_categories.append('True Positive')\n","    elif pred == 'Low' and act == 'Low':\n","        pred_categories.append('True Negative')\n","    elif pred == 'High' and act == 'Low':\n","        pred_categories.append('False Positive')\n","    else:  # pred == 'Low' and act == 'High'\n","        pred_categories.append('False Negative')\n","\n","# Format FIPS as 5-digit string\n","fips_formatted = data_full['FIPS'].astype(str).str.zfill(5)\n","\n","# Create map data\n","map_data = pd.DataFrame({\n","    'FIPS': fips_formatted,\n","    'County': data_full['County'],\n","    'State': data_full['State'],\n","    'Actual_Risk': actual,\n","    'Predicted_Risk': predictions,\n","    'Prediction_Probability': probabilities,\n","    'Prediction_Category': pred_categories,\n","    'Is_Correct': correct,\n","    'Food_Access_Barrier_Index': data_full['Food_Access_Barrier_Index'],\n","    'Obesity_Rate': data_full['% Adults with Obesity'],\n","    'Diabetes_Rate': data_full['% Adults with Diabetes'],\n","    'Poverty_Rate': data_full['% Children in Poverty'],\n","    'Income_80th': data_full['80th Percentile Income'],\n","    'High_School_Completion': data_full['% Completed High School']\n","})\n","\n","# Save\n","map_data.to_csv('map_prediction_data.csv', index=False)\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"MAP DATA GENERATION COMPLETE\")\n","print(\"=\"*70)\n","print(f\"\\nSaved: map_prediction_data.csv\")\n","print(f\"Total counties: {len(map_data)}\")\n","print(f\"\\nPrediction accuracy breakdown:\")\n","print(f\"  True Positives: {sum(map_data['Prediction_Category']=='True Positive')}\")\n","print(f\"  True Negatives: {sum(map_data['Prediction_Category']=='True Negative')}\")\n","print(f\"  False Positives: {sum(map_data['Prediction_Category']=='False Positive')}\")\n","print(f\"  False Negatives: {sum(map_data['Prediction_Category']=='False Negative')}\")\n","print(f\"  Overall Accuracy: {sum(correct)/len(correct):.2%}\")\n","print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vXdHHRrfbFOi","executionInfo":{"status":"ok","timestamp":1763838244640,"user_tz":480,"elapsed":1987,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"7e6f1ab9-983e-4269-aeb1-c075b9d55ea2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","GENERATING MAP PREDICTION DATA\n","======================================================================\n","\n","Training samples: 1519\n","Total counties: 2236\n","\n","Training Random Forest...\n","\n","======================================================================\n","MAP DATA GENERATION COMPLETE\n","======================================================================\n","\n","Saved: map_prediction_data.csv\n","Total counties: 2236\n","\n","Prediction accuracy breakdown:\n","  True Positives: 1001\n","  True Negatives: 982\n","  False Positives: 119\n","  False Negatives: 134\n","  Overall Accuracy: 88.69%\n","======================================================================\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Generate Complete Dashboard\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","import plotly.graph_objects as go\n","import plotly.express as px\n","from datetime import datetime\n","import json\n","import urllib.request\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"=\"*70)\n","print(\"GENERATING COMPLETE DASHBOARD WITH LATEST DATA\")\n","print(\"=\"*70)\n","\n","# Load all data\n","print(\"\\n[1/8] Loading all result files...\")\n","\n","# Step 1: Three-class\n","step1_df = pd.read_csv('step1_model_comparison_results.csv')\n","step1_df = step1_df[step1_df['Feature_Set'] == 'Set1_Original'].reset_index(drop=True)\n","cm_rf_three = pd.read_csv('step1_confusion_matrix_rf.csv', header=None).values\n","cm_et_three = pd.read_csv('step1_confusion_matrix_et.csv', header=None).values\n","gini_df = pd.read_csv('step1_gini_comparison.csv')\n","\n","# Step 2: Binary\n","step2_df = pd.read_csv('step2_model_comparison_binary.csv')\n","step2_df = step2_df[step2_df['Feature_Set'] == 'Set1_Original'].reset_index(drop=True)\n","cm_rf_binary = pd.read_csv('step2_confusion_matrix_rf_binary.csv', header=None).values\n","cm_et_binary = pd.read_csv('step2_confusion_matrix_et_binary.csv', header=None).values\n","\n","# Step 3: DBSCAN\n","step3_df = pd.read_csv('step3_model_results_dbscan.csv')\n","cm_rf_dbscan = pd.read_csv('step3_confusion_matrix_rf.csv', header=None).values\n","cm_et_dbscan = pd.read_csv('step3_confusion_matrix_et.csv', header=None).values\n","feature_importance = pd.read_csv('step3_feature_importance.csv')\n","\n","# Step 4: Ensemble\n","step4_df = pd.read_csv('step4_ensemble_comparison.csv')\n","cm_ensemble = pd.read_csv('step4_confusion_matrix_ensemble.csv', header=None).values\n","\n","# Maps and timing\n","map_data = pd.read_csv('map_prediction_data.csv')\n","timing_data = pd.read_csv('training_time_comparison.csv')\n","\n","print(\"  All files loaded successfully!\")\n","\n","# Download GeoJSON\n","print(\"\\n[2/8] Downloading GeoJSON...\")\n","try:\n","    geojson_url = \"https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json\"\n","    with urllib.request.urlopen(geojson_url) as response:\n","        counties = json.loads(response.read())\n","    print(\"  GeoJSON downloaded\")\n","except:\n","    print(\"  Warning: GeoJSON download failed\")\n","    counties = None\n","\n","# Create Gini chart\n","print(\"\\n[3/8] Creating Gini Index chart...\")\n","fig_gini = go.Figure()\n","fig_gini.add_trace(go.Bar(\n","    x=gini_df['Classification_Type'],\n","    y=gini_df['Gini_Index_Test'],\n","    marker_color=['#e74c3c', '#27ae60'],\n","    text=[f'{v:.4f}' for v in gini_df['Gini_Index_Test']],\n","    textposition='outside'\n","))\n","fig_gini.update_layout(\n","    title='Class Purity: Gini Index Comparison',\n","    yaxis_title='Gini Index (lower is better)',\n","    yaxis=dict(range=[0, 0.75]),\n","    height=500\n",")\n","gini_html = fig_gini.to_html(full_html=False, include_plotlyjs='cdn')\n","\n","# Create Runtime chart\n","print(\"\\n[4/8] Creating runtime chart...\")\n","\n","# Check actual column names\n","timing_cols = timing_data.columns.tolist()\n","print(f\"  Timing columns: {timing_cols}\")\n","\n","# Flexible column name detection\n","time_col = next((col for col in timing_cols if 'time' in col.lower() and 'train' in col.lower()), timing_cols[1] if len(timing_cols) > 1 else None)\n","pred_col = next((col for col in timing_cols if 'time' in col.lower() and 'pred' in col.lower()), timing_cols[2] if len(timing_cols) > 2 else None)\n","model_col = timing_cols[0]\n","\n","fig_runtime = go.Figure()\n","if time_col:\n","    fig_runtime.add_trace(go.Bar(\n","        name='Training Time',\n","        x=timing_data[model_col],\n","        y=timing_data[time_col],\n","        marker_color='#2E86AB',\n","        text=[f'{t:.2f}s' for t in timing_data[time_col]],\n","        textposition='auto'\n","    ))\n","if pred_col:\n","    fig_runtime.add_trace(go.Bar(\n","        name='Prediction Time',\n","        x=timing_data[model_col],\n","        y=timing_data[pred_col],\n","        marker_color='#A23B72',\n","        text=[f'{t:.3f}s' for t in timing_data[pred_col]],\n","        textposition='auto'\n","    ))\n","fig_runtime.update_layout(\n","    title='Model Runtime Performance Comparison',\n","    xaxis_title='Model',\n","    yaxis_title='Time (seconds)',\n","    barmode='group',\n","    height=400\n",")\n","runtime_html = fig_runtime.to_html(full_html=False, include_plotlyjs=False)\n","\n","# Create DBSCAN scatter plot\n","print(\"\\n[5/8] Creating DBSCAN scatter plot...\")\n","try:\n","    dbscan_filter = pd.read_csv('step3_dbscan_filtering_results.csv')\n","\n","    fig_dbscan = go.Figure()\n","\n","    # Plot clean samples\n","    clean_data = dbscan_filter[dbscan_filter['Cluster'] != -1]\n","    fig_dbscan.add_trace(go.Scatter(\n","        x=clean_data['PC1'],\n","        y=clean_data['PC2'],\n","        mode='markers',\n","        name='Clean Data',\n","        marker=dict(color='#2ecc71', size=5, opacity=0.6)\n","    ))\n","\n","    # Plot noise samples\n","    noise_data = dbscan_filter[dbscan_filter['Cluster'] == -1]\n","    fig_dbscan.add_trace(go.Scatter(\n","        x=noise_data['PC1'],\n","        y=noise_data['PC2'],\n","        mode='markers',\n","        name='Noise (Removed)',\n","        marker=dict(color='#e74c3c', size=7, opacity=0.8, symbol='x')\n","    ))\n","\n","    fig_dbscan.update_layout(\n","        title=f'DBSCAN Noise Detection: {len(noise_data)} samples removed',\n","        xaxis_title='Principal Component 1',\n","        yaxis_title='Principal Component 2',\n","        height=500\n","    )\n","\n","    dbscan_html = fig_dbscan.to_html(full_html=False, include_plotlyjs=False)\n","    print(f\"  DBSCAN scatter plot created ({len(noise_data)} noise samples)\")\n","except Exception as e:\n","    print(f\"  Warning: Could not create DBSCAN plot: {e}\")\n","    dbscan_html = \"<p>DBSCAN visualization not available</p>\"\n","\n","# Create Maps\n","print(\"\\n[6/8] Creating interactive maps...\")\n","\n","if counties is not None:\n","    # Map 1: Actual Risk\n","    fig_map1 = px.choropleth(\n","        map_data, geojson=counties, locations='FIPS',\n","        color='Actual_Risk', scope=\"usa\",\n","        color_discrete_map={'High': '#e74c3c', 'Low': '#27ae60'},\n","        labels={'Actual_Risk': 'Health Risk'},\n","        hover_data={'County': True, 'State': True, 'FIPS': False, 'Obesity_Rate': ':.1f'}\n","    )\n","    fig_map1.update_layout(title='Actual Health Risk Distribution', height=500, width=None, margin=dict(l=0, r=0, t=40, b=0))\n","    fig_map1.update_geos(fitbounds=\"locations\", visible=False)\n","\n","    # Map 2: Predicted Risk\n","    fig_map2 = px.choropleth(\n","        map_data, geojson=counties, locations='FIPS',\n","        color='Predicted_Risk', scope=\"usa\",\n","        color_discrete_map={'High': '#e74c3c', 'Low': '#27ae60'},\n","        labels={'Predicted_Risk': 'Predicted Risk'},\n","        hover_data={'County': True, 'State': True, 'FIPS': False}\n","    )\n","    fig_map2.update_layout(title='Model Predictions (Best Ensemble Method)', height=500, width=None, margin=dict(l=0, r=0, t=40, b=0))\n","    fig_map2.update_geos(fitbounds=\"locations\", visible=False)\n","\n","    # Map 3: Prediction Accuracy\n","    map_data['Correct'] = (map_data['Actual_Risk'] == map_data['Predicted_Risk']).astype(int)\n","    map_data['Accuracy_Label'] = map_data['Correct'].map({1: 'Correct', 0: 'Incorrect'})\n","\n","    fig_map3 = px.choropleth(\n","        map_data, geojson=counties, locations='FIPS',\n","        color='Accuracy_Label', scope=\"usa\",\n","        color_discrete_map={'Correct': '#27ae60', 'Incorrect': '#e74c3c'},\n","        category_orders={'Accuracy_Label': ['Correct', 'Incorrect']},\n","        labels={'Accuracy_Label': 'Prediction'},\n","        hover_data={'County': True, 'State': True, 'Actual_Risk': True, 'Predicted_Risk': True, 'FIPS': False}\n","    )\n","    fig_map3.update_layout(title='Prediction Accuracy (Green=Correct, Red=Incorrect)', height=500, width=None, margin=dict(l=0, r=0, t=40, b=0))\n","    fig_map3.update_geos(fitbounds=\"locations\", visible=False)\n","\n","    # Map 4: Food Desert\n","    fig_map4 = px.choropleth(\n","        map_data, geojson=counties, locations='FIPS',\n","        color='Food_Access_Barrier_Index', scope=\"usa\",\n","        color_continuous_scale='Reds',\n","        labels={'Food_Access_Barrier_Index': 'Food Desert Severity'},\n","        hover_data={'County': True, 'State': True, 'FIPS': False}\n","    )\n","    fig_map4.update_layout(title='Food Access Barrier Distribution', height=500, width=None, margin=dict(l=0, r=0, t=40, b=0))\n","    fig_map4.update_geos(fitbounds=\"locations\", visible=False)\n","\n","    map1_html = fig_map1.to_html(full_html=False, include_plotlyjs=False)\n","    map2_html = fig_map2.to_html(full_html=False, include_plotlyjs=False)\n","    map3_html = fig_map3.to_html(full_html=False, include_plotlyjs=False)\n","    map4_html = fig_map4.to_html(full_html=False, include_plotlyjs=False)\n","\n","    print(\"  4 interactive maps created\")\n","    maps_available = True\n","else:\n","    map1_html = map2_html = map3_html = map4_html = '<p>Maps not available</p>'\n","    maps_available = False\n","\n","# Calculate metrics\n","print(\"\\n[6/8] Calculating key metrics...\")\n","\n","best_method = step4_df.iloc[0]['Method']\n","best_accuracy = step4_df.iloc[0]['Accuracy']\n","initial_accuracy = step1_df['Accuracy'].mean()\n","total_improvement = best_accuracy - initial_accuracy\n","\n","# Food desert rank\n","sorted_features = feature_importance.sort_values('RF_Importance', ascending=False).reset_index(drop=True)\n","food_desert_row = sorted_features[sorted_features['Feature']=='Food_Access_Barrier_Index']\n","if not food_desert_row.empty:\n","    food_desert_rank = food_desert_row.index[0] + 1\n","    food_desert_importance = food_desert_row['RF_Importance'].values[0]\n","else:\n","    food_desert_rank = 999\n","    food_desert_importance = 0.0\n","\n","test_accuracy = best_accuracy\n","full_accuracy = (map_data['Actual_Risk'] == map_data['Predicted_Risk']).sum() / len(map_data)\n","\n","# Build HTML\n","print(\"\\n[7/8] Building HTML dashboard...\")\n","\n","html_content = f'''<!DOCTYPE html>\n","<html lang=\"en\">\n","<head>\n","    <meta charset=\"UTF-8\">\n","    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n","    <title>Health Risk Prediction Analysis - Complete Dashboard</title>\n","    <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n","    <style>\n","        * {{\n","            margin: 0;\n","            padding: 0;\n","            box-sizing: border-box;\n","        }}\n","\n","        body {{\n","            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n","            line-height: 1.6;\n","            color: #333;\n","            background: #f5f5f5;\n","        }}\n","\n","        .container {{\n","            max-width: 1400px;\n","            margin: 0 auto;\n","            padding: 20px;\n","        }}\n","\n","        header {{\n","            background: linear-gradient(135deg, #2E86AB 0%, #A23B72 100%);\n","            color: white;\n","            padding: 40px 20px;\n","            text-align: center;\n","            margin-bottom: 30px;\n","            border-radius: 10px;\n","            box-shadow: 0 4px 6px rgba(0,0,0,0.1);\n","        }}\n","\n","        h1 {{\n","            font-size: 2.5em;\n","            margin-bottom: 10px;\n","        }}\n","\n","        .subtitle {{\n","            font-size: 1.2em;\n","            opacity: 0.9;\n","        }}\n","\n","        .section {{\n","            background: white;\n","            padding: 30px;\n","            margin-bottom: 30px;\n","            border-radius: 10px;\n","            box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n","        }}\n","\n","        h2 {{\n","            color: #2E86AB;\n","            margin-bottom: 20px;\n","            padding-bottom: 10px;\n","            border-bottom: 3px solid #2E86AB;\n","        }}\n","\n","        h3 {{\n","            color: #555;\n","            margin: 20px 0 10px 0;\n","        }}\n","\n","        .metrics-grid {{\n","            display: grid;\n","            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));\n","            gap: 20px;\n","            margin: 20px 0;\n","        }}\n","\n","        .metric-card {{\n","            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n","            color: white;\n","            padding: 20px;\n","            border-radius: 8px;\n","            text-align: center;\n","        }}\n","\n","        .metric-value {{\n","            font-size: 2em;\n","            font-weight: bold;\n","            margin: 10px 0;\n","        }}\n","\n","        .metric-label {{\n","            font-size: 0.9em;\n","            opacity: 0.9;\n","        }}\n","\n","        .highlight {{\n","            background: #fff3cd;\n","            border-left: 4px solid #ffc107;\n","            padding: 15px;\n","            margin: 20px 0;\n","            border-radius: 4px;\n","        }}\n","\n","        .success {{\n","            background: #d4edda;\n","            border-left: 4px solid #28a745;\n","            padding: 15px;\n","            margin: 20px 0;\n","            border-radius: 4px;\n","        }}\n","\n","        table.comparison-table {{\n","            width: 100%;\n","            border-collapse: collapse;\n","            margin: 20px 0;\n","        }}\n","\n","        table.comparison-table th,\n","        table.comparison-table td {{\n","            padding: 12px;\n","            text-align: left;\n","            border-bottom: 1px solid #ddd;\n","        }}\n","\n","        table.comparison-table th {{\n","            background: #f8f9fa;\n","            font-weight: 600;\n","        }}\n","\n","        table.cm-table {{\n","            border-collapse: collapse;\n","            margin: 10px 0;\n","        }}\n","\n","        table.cm-table th,\n","        table.cm-table td {{\n","            padding: 8px 12px;\n","            border: 1px solid #ddd;\n","            text-align: center;\n","        }}\n","\n","        table.cm-table th {{\n","            background: #e9ecef;\n","        }}\n","\n","        .map-btn {{\n","            padding: 10px 20px;\n","            background: #ddd;\n","            color: #333;\n","            border: none;\n","            border-radius: 5px;\n","            cursor: pointer;\n","            font-size: 14px;\n","            margin: 5px;\n","            transition: all 0.3s;\n","        }}\n","\n","        .map-btn.active {{\n","            background: #2E86AB;\n","            color: white;\n","        }}\n","\n","        .map-btn:hover {{\n","            background: #1a5570;\n","            color: white;\n","        }}\n","\n","        .map-container {{\n","            display: none;\n","            width: 100% !important;\n","        }}\n","\n","        .map-container.active {{\n","            display: block;\n","            width: 100% !important;\n","        }}\n","\n","        .map-container .plotly {{\n","            width: 100% !important;\n","        }}\n","\n","        .map-container .js-plotly-plot {{\n","            width: 100% !important;\n","        }}\n","    </style>\n","</head>\n","<body>\n","    <div class=\"container\">\n","        <header>\n","            <h1>Health Risk Prediction Analysis</h1>\n","            <p class=\"subtitle\">Random Forest vs Extra Trees: County-Level Metabolic Health Risk Prediction</p>\n","            <p class=\"subtitle\" style=\"font-size: 1em; margin-top: 10px;\">Food Desert Impact Analysis</p>\n","        </header>\n","\n","        <!-- EXECUTIVE SUMMARY -->\n","        <div class=\"section\">\n","            <h2>Executive Summary</h2>\n","            <div class=\"metrics-grid\">\n","                <div class=\"metric-card\">\n","                    <div class=\"metric-label\">Best Method</div>\n","                    <div class=\"metric-value\" style=\"font-size: 1.3em;\">{best_method}</div>\n","                </div>\n","                <div class=\"metric-card\">\n","                    <div class=\"metric-label\">Final Accuracy</div>\n","                    <div class=\"metric-value\">{best_accuracy:.1%}</div>\n","                </div>\n","                <div class=\"metric-card\">\n","                    <div class=\"metric-label\">Total Improvement</div>\n","                    <div class=\"metric-value\">+{total_improvement*100:.1f}%</div>\n","                </div>\n","                <div class=\"metric-card\">\n","                    <div class=\"metric-label\">Food Desert Rank</div>\n","                    <div class=\"metric-value\">#{food_desert_rank}</div>\n","                    <div class=\"metric-label\">{food_desert_importance:.1%} Importance</div>\n","                </div>\n","            </div>\n","\n","            <div class=\"success\">\n","                <strong>Key Finding:</strong> Food_Access_Barrier_Index ranks #{food_desert_rank} with {food_desert_importance:.1%} importance,\n","                demonstrating that food deserts have a significant independent impact on metabolic health risk.\n","            </div>\n","        </div>\n","\n","        <!-- STEP 1: THREE-CLASS -->\n","        <div class=\"section\">\n","            <h2>Step 1: Three-Class Classification Baseline</h2>\n","\n","            <h3>Approach</h3>\n","            <p>Initial three-class classification: Low, Medium, High risk based on Health Risk Score tertiles.</p>\n","\n","            <h3>Results</h3>\n","            <table class=\"comparison-table\">\n","                <thead>\n","                    <tr>\n","                        <th>Model</th>\n","                        <th>Accuracy</th>\n","                        <th>F1-Weighted</th>\n","                        <th>F1-Macro</th>\n","                        <th>Cohen Kappa</th>\n","                    </tr>\n","                </thead>\n","                <tbody>\n","'''\n","\n","for _, row in step1_df.iterrows():\n","    html_content += f'''\n","                    <tr>\n","                        <td>{row['Model']}</td>\n","                        <td>{row['Accuracy']:.4f}</td>\n","                        <td>{row['F1_Weighted']:.4f}</td>\n","                        <td>{row['F1_Macro']:.4f}</td>\n","                        <td>{row['Cohens_Kappa']:.4f}</td>\n","                    </tr>\n","'''\n","\n","html_content += f'''\n","                </tbody>\n","            </table>\n","\n","            <h3>Confusion Matrices</h3>\n","            <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px;\">\n","                <div>\n","                    <h4>Random Forest</h4>\n","                    <table class=\"cm-table\">\n","                        <thead><tr><th></th><th>Pred High</th><th>Pred Low</th><th>Pred Medium</th></tr></thead>\n","                        <tbody>\n","                            <tr><th>True High</th><td>{cm_rf_three[0,0]}</td><td>{cm_rf_three[0,1]}</td><td>{cm_rf_three[0,2]}</td></tr>\n","                            <tr><th>True Low</th><td>{cm_rf_three[1,0]}</td><td>{cm_rf_three[1,1]}</td><td>{cm_rf_three[1,2]}</td></tr>\n","                            <tr><th>True Medium</th><td>{cm_rf_three[2,0]}</td><td>{cm_rf_three[2,1]}</td><td>{cm_rf_three[2,2]}</td></tr>\n","                        </tbody>\n","                    </table>\n","                </div>\n","                <div>\n","                    <h4>Extra Trees</h4>\n","                    <table class=\"cm-table\">\n","                        <thead><tr><th></th><th>Pred High</th><th>Pred Low</th><th>Pred Medium</th></tr></thead>\n","                        <tbody>\n","                            <tr><th>True High</th><td>{cm_et_three[0,0]}</td><td>{cm_et_three[0,1]}</td><td>{cm_et_three[0,2]}</td></tr>\n","                            <tr><th>True Low</th><td>{cm_et_three[1,0]}</td><td>{cm_et_three[1,1]}</td><td>{cm_et_three[1,2]}</td></tr>\n","                            <tr><th>True Medium</th><td>{cm_et_three[2,0]}</td><td>{cm_et_three[2,1]}</td><td>{cm_et_three[2,2]}</td></tr>\n","                        </tbody>\n","                    </table>\n","                </div>\n","            </div>\n","\n","            <div class=\"highlight\">\n","                <strong>Problem Identified:</strong> Poor performance (~55% accuracy) due to ambiguous \"Medium\" class boundaries.\n","                The model struggled to distinguish between Medium and High/Low categories, leading to significant misclassification.\n","                Gini Index of 0.67 indicates high impurity and poor class separation.\n","            </div>\n","        </div>\n","\n","        <!-- GINI INDEX -->\n","        <div class=\"section\">\n","            <h2>Class Purity Analysis: Gini Index</h2>\n","            <p>Gini Index quantifies class purity. Lower values indicate cleaner class separation.</p>\n","\n","            {gini_html}\n","\n","            <div class=\"success\">\n","                <strong>Insight:</strong> Binary classification has significantly lower Gini Index ({gini_df.iloc[1]['Gini_Index_Test']:.4f} vs {gini_df.iloc[0]['Gini_Index_Test']:.4f}),\n","                explaining why it outperforms three-class classification.\n","            </div>\n","        </div>\n","\n","        <!-- STEP 2: BINARY -->\n","        <div class=\"section\">\n","            <h2>Step 2: Binary Classification (Improved)</h2>\n","\n","            <h3>Approach</h3>\n","            <p>Simplified to binary classification (High vs Low risk) using median obesity rate as threshold.</p>\n","\n","            <h3>Results</h3>\n","            <table class=\"comparison-table\">\n","                <thead>\n","                    <tr>\n","                        <th>Model</th>\n","                        <th>Accuracy</th>\n","                        <th>F1-Score</th>\n","                        <th>ROC-AUC</th>\n","                        <th>Improvement</th>\n","                    </tr>\n","                </thead>\n","                <tbody>\n","'''\n","\n","for _, row in step2_df.iterrows():\n","    improvement = row['Accuracy'] - step1_df[step1_df['Model']==row['Model']]['Accuracy'].values[0]\n","    html_content += f'''\n","                    <tr>\n","                        <td>{row['Model']}</td>\n","                        <td>{row['Accuracy']:.4f}</td>\n","                        <td>{row['F1_Score']:.4f}</td>\n","                        <td>{row['ROC_AUC']:.4f}</td>\n","                        <td style=\"color: green;\">+{improvement*100:.1f}%</td>\n","                    </tr>\n","'''\n","\n","html_content += f'''\n","                </tbody>\n","            </table>\n","\n","            <h3>Confusion Matrices</h3>\n","            <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px;\">\n","                <div>\n","                    <h4>Random Forest</h4>\n","                    <table class=\"cm-table\">\n","                        <thead><tr><th></th><th>Pred High</th><th>Pred Low</th></tr></thead>\n","                        <tbody>\n","                            <tr><th>True High</th><td>{cm_rf_binary[0,0]}</td><td>{cm_rf_binary[0,1]}</td></tr>\n","                            <tr><th>True Low</th><td>{cm_rf_binary[1,0]}</td><td>{cm_rf_binary[1,1]}</td></tr>\n","                        </tbody>\n","                    </table>\n","                </div>\n","                <div>\n","                    <h4>Extra Trees</h4>\n","                    <table class=\"cm-table\">\n","                        <thead><tr><th></th><th>Pred High</th><th>Pred Low</th></tr></thead>\n","                        <tbody>\n","                            <tr><th>True High</th><td>{cm_et_binary[0,0]}</td><td>{cm_et_binary[0,1]}</td></tr>\n","                            <tr><th>True Low</th><td>{cm_et_binary[1,0]}</td><td>{cm_et_binary[1,1]}</td></tr>\n","                        </tbody>\n","                    </table>\n","                </div>\n","            </div>\n","\n","            <div class=\"success\">\n","                <strong>Achievement:</strong> Significant improvement of ~11-12% accuracy by eliminating ambiguous \"Medium\" class.\n","            </div>\n","        </div>\n","\n","        <!-- STEP 3: DBSCAN -->\n","        <div class=\"section\">\n","            <h2>Step 3: DBSCAN Noise Filtering</h2>\n","\n","            <h3>Approach</h3>\n","            <p>Applied DBSCAN clustering to identify and remove noisy samples from training data.</p>\n","\n","            <h3>Results</h3>\n","            <table class=\"comparison-table\">\n","                <thead>\n","                    <tr>\n","                        <th>Model</th>\n","                        <th>Training Samples</th>\n","                        <th>Accuracy</th>\n","                        <th>F1-Score</th>\n","                        <th>ROC-AUC</th>\n","                    </tr>\n","                </thead>\n","                <tbody>\n","'''\n","\n","for _, row in step3_df.iterrows():\n","    html_content += f'''\n","                    <tr>\n","                        <td>{row['Model']}</td>\n","                        <td>{row['Train_Samples']}</td>\n","                        <td>{row['Accuracy']:.4f}</td>\n","                        <td>{row['F1_Score']:.4f}</td>\n","                        <td>{row['ROC_AUC']:.4f}</td>\n","                    </tr>\n","'''\n","\n","html_content += f'''\n","                </tbody>\n","            </table>\n","\n","            <h3>DBSCAN Noise Detection Visualization</h3>\n","            <p>Visualization of clean samples vs noise detected by DBSCAN clustering in PC1-PC2 space.</p>\n","            {dbscan_html}\n","\n","            <h3>Confusion Matrices</h3>\n","            <div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px;\">\n","                <div>\n","                    <h4>Random Forest</h4>\n","                    <table class=\"cm-table\">\n","                        <thead><tr><th></th><th>Pred High</th><th>Pred Low</th></tr></thead>\n","                        <tbody>\n","                            <tr><th>True High</th><td>{cm_rf_dbscan[0,0]}</td><td>{cm_rf_dbscan[0,1]}</td></tr>\n","                            <tr><th>True Low</th><td>{cm_rf_dbscan[1,0]}</td><td>{cm_rf_dbscan[1,1]}</td></tr>\n","                        </tbody>\n","                    </table>\n","                </div>\n","                <div>\n","                    <h4>Extra Trees</h4>\n","                    <table class=\"cm-table\">\n","                        <thead><tr><th></th><th>Pred High</th><th>Pred Low</th></tr></thead>\n","                        <tbody>\n","                            <tr><th>True High</th><td>{cm_et_dbscan[0,0]}</td><td>{cm_et_dbscan[0,1]}</td></tr>\n","                            <tr><th>True Low</th><td>{cm_et_dbscan[1,0]}</td><td>{cm_et_dbscan[1,1]}</td></tr>\n","                        </tbody>\n","                    </table>\n","                </div>\n","            </div>\n","\n","            <div class=\"success\">\n","                <strong>Data Quality:</strong> DBSCAN filtering maintained stable ~66-68% performance with cleaner data.\n","            </div>\n","        </div>\n","\n","        <!-- STEP 4: ENSEMBLE -->\n","        <div class=\"section\">\n","            <h2>Step 4: Ensemble Methods Comparison</h2>\n","\n","            <h3>Approach</h3>\n","            <p>Evaluated advanced ensemble techniques to improve prediction accuracy.</p>\n","\n","            <h3>Results</h3>\n","            <table class=\"comparison-table\">\n","                <thead>\n","                    <tr>\n","                        <th>Method</th>\n","                        <th>Accuracy</th>\n","                        <th>F1-Score</th>\n","                        <th>ROC-AUC</th>\n","                    </tr>\n","                </thead>\n","                <tbody>\n","'''\n","\n","for _, row in step4_df.iterrows():\n","    html_content += f'''\n","                    <tr>\n","                        <td>{row['Method']}</td>\n","                        <td>{row['Accuracy']:.4f}</td>\n","                        <td>{row['F1_Score']:.4f}</td>\n","                        <td>{row['ROC_AUC']:.4f}</td>\n","                    </tr>\n","'''\n","\n","html_content += f'''\n","                </tbody>\n","            </table>\n","\n","            <h3>Best Model Confusion Matrix</h3>\n","            <div style=\"max-width: 400px; margin: 0 auto;\">\n","                <table class=\"cm-table\">\n","                    <thead><tr><th></th><th>Pred High</th><th>Pred Low</th></tr></thead>\n","                    <tbody>\n","                        <tr><th>True High</th><td>{cm_ensemble[0,0]}</td><td>{cm_ensemble[0,1]}</td></tr>\n","                        <tr><th>True Low</th><td>{cm_ensemble[1,0]}</td><td>{cm_ensemble[1,1]}</td></tr>\n","                    </tbody>\n","                </table>\n","            </div>\n","\n","            <div class=\"success\">\n","                <strong>Best Result:</strong> {best_method} achieved {best_accuracy:.1%} accuracy on test set.\n","            </div>\n","        </div>\n","\n","        <!-- RUNTIME PERFORMANCE -->\n","        <div class=\"section\">\n","            <h2>Model Runtime Performance</h2>\n","            <p>Comparison of training and prediction times across different models.</p>\n","\n","            {runtime_html}\n","\n","            <div class=\"highlight\">\n","                <strong>Performance Note:</strong> While {best_method} achieved the highest accuracy ({best_accuracy:.1%}),\n","                model selection should consider the trade-off between accuracy and computational cost.\n","            </div>\n","        </div>\n","\n","        <!-- INTERACTIVE MAPS -->\n","        <div class=\"section\">\n","            <h2>Interactive County-Level Maps</h2>\n","            <p>Explore geographic patterns of health risk, model predictions, and food access barriers across all US counties.</p>\n","\n","            <div style=\"display: flex; gap: 10px; margin-bottom: 20px; flex-wrap: wrap; justify-content: center;\">\n","                <button class=\"map-btn active\" onclick=\"showMap('map1')\">Actual Risk</button>\n","                <button class=\"map-btn\" onclick=\"showMap('map2')\">Predicted Risk</button>\n","                <button class=\"map-btn\" onclick=\"showMap('map3')\">Prediction Accuracy</button>\n","                <button class=\"map-btn\" onclick=\"showMap('map4')\">Food Desert</button>\n","            </div>\n","\n","            <div id=\"map1\" class=\"map-container active\">\n","                {map1_html}\n","            </div>\n","            <div id=\"map2\" class=\"map-container\">\n","                {map2_html}\n","            </div>\n","            <div id=\"map3\" class=\"map-container\">\n","                {map3_html}\n","            </div>\n","            <div id=\"map4\" class=\"map-container\">\n","                {map4_html}\n","            </div>\n","\n","            <div class=\"highlight\" style=\"margin-top: 20px;\">\n","                <strong>Model Performance Context:</strong><br>\n","                <ul style=\"margin-left: 20px; margin-top: 10px;\">\n","                    <li><strong>Test Set Accuracy:</strong> {test_accuracy:.1%} (456 counties never seen during training)</li>\n","                    <li><strong>Full Data Visualization:</strong> {full_accuracy:.1%} (2,236 counties including training data)</li>\n","                    <li><strong>Note:</strong> Maps show predictions for all counties. Higher accuracy on full data is expected since the model was trained on a subset.</li>\n","                </ul>\n","            </div>\n","        </div>\n","\n","        <!-- FEATURE IMPORTANCE -->\n","        <div class=\"section\">\n","            <h2>Feature Importance Analysis</h2>\n","\n","            <h3>Top 12 Features</h3>\n","            <table class=\"comparison-table\">\n","                <thead>\n","                    <tr>\n","                        <th>Rank</th>\n","                        <th>Feature</th>\n","                        <th>RF Importance</th>\n","                        <th>ET Importance</th>\n","                    </tr>\n","                </thead>\n","                <tbody>\n","'''\n","\n","for idx, row in sorted_features.head(12).iterrows():\n","    rank = idx + 1\n","    html_content += f'''\n","                    <tr{'style=\"background: #fff3cd;\"' if 'Food_Access' in row['Feature'] else ''}>\n","                        <td>#{rank}</td>\n","                        <td>{row['Feature']}</td>\n","                        <td>{row['RF_Importance']:.4f}</td>\n","                        <td>{row['ET_Importance']:.4f}</td>\n","                    </tr>\n","'''\n","\n","html_content += f'''\n","                </tbody>\n","            </table>\n","\n","            <div class=\"success\">\n","                <strong>Research Hypothesis Confirmed:</strong> Food_Access_Barrier_Index ranks #{food_desert_rank}\n","                with {food_desert_importance:.1%} importance, demonstrating significant independent predictive power\n","                for metabolic health outcomes.\n","            </div>\n","        </div>\n","\n","        <!-- CONCLUSIONS -->\n","        <div class=\"section\">\n","            <h2>Conclusions and Key Findings</h2>\n","\n","            <h3>1. Model Performance Evolution</h3>\n","            <p>Systematic optimization achieved substantial improvements:</p>\n","            <ul style=\"margin-left: 20px;\">\n","                <li><strong>Step 1 (Three-Class):</strong> ~55% accuracy - Failed due to poor class separation (Gini=0.67)</li>\n","                <li><strong>Step 2 (Binary):</strong> ~66% accuracy - +11% improvement through problem reformulation</li>\n","                <li><strong>Step 3 (DBSCAN):</strong> ~67% accuracy - Maintained performance with cleaner data</li>\n","                <li><strong>Step 4 (Ensemble):</strong> {best_accuracy:.1%} accuracy - Best result with {best_method}</li>\n","            </ul>\n","\n","            <h3>2. Food Desert Impact Validation</h3>\n","            <p>Food_Access_Barrier_Index emerged as a top predictor (#{food_desert_rank}, {food_desert_importance:.1%} importance),\n","            confirming that food deserts independently contribute to metabolic health risk.</p>\n","\n","            <h3>3. Methodological Insights</h3>\n","            <ul style=\"margin-left: 20px;\">\n","                <li><strong>Problem Formulation:</strong> Binary classification vastly outperformed three-class approach</li>\n","                <li><strong>Data Quality:</strong> DBSCAN noise removal improved model reliability</li>\n","                <li><strong>Ensemble Methods:</strong> Advanced techniques achieved marginal improvements</li>\n","                <li><strong>Feature Engineering:</strong> Domain-relevant features show strong predictive power</li>\n","            </ul>\n","\n","            <h3>4. Public Health Implications</h3>\n","            <p>The strong predictive performance of Food_Access_Barrier_Index suggests that improving food access\n","            infrastructure could be an effective intervention strategy for reducing metabolic disease burden.</p>\n","        </div>\n","\n","        <footer style=\"text-align: center; padding: 20px; color: #666;\">\n","            <p>Health Risk Prediction Analysis Dashboard</p>\n","            <p>Generated on {datetime.now().strftime('%B %d, %Y at %H:%M:%S')}</p>\n","        </footer>\n","    </div>\n","\n","    <script>\n","        function showMap(mapId) {{\n","            // Hide all maps\n","            document.querySelectorAll('.map-container').forEach(container => {{\n","                container.style.display = 'none';\n","                container.classList.remove('active');\n","            }});\n","\n","            // Remove active from all buttons\n","            document.querySelectorAll('.map-btn').forEach(btn => {{\n","                btn.classList.remove('active');\n","            }});\n","\n","            // Show selected map\n","            var selectedMap = document.getElementById(mapId);\n","            selectedMap.style.display = 'block';\n","            selectedMap.classList.add('active');\n","\n","            // Highlight button\n","            event.target.classList.add('active');\n","\n","            // Force Plotly to resize the map to full width\n","            setTimeout(function() {{\n","                var plotlyDivs = selectedMap.querySelectorAll('.js-plotly-plot');\n","                plotlyDivs.forEach(function(div) {{\n","                    Plotly.Plots.resize(div);\n","                }});\n","            }}, 100);\n","        }}\n","    </script>\n","</body>\n","</html>\n","'''\n","\n","# Save\n","print(\"\\n[8/8] Saving dashboard...\")\n","output_file = 'health_risk_analysis_dashboard_COMPLETE.html'\n","with open(output_file, 'w', encoding='utf-8') as f:\n","    f.write(html_content)\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"COMPLETE DASHBOARD GENERATED!\")\n","print(\"=\"*70)\n","print(f\"\\nOutput file: {output_file}\")\n","print(f\"File size: {len(html_content)/1024:.1f} KB\")\n","print(\"\\nDashboard includes:\")\n","print(\"  - Executive Summary with key metrics\")\n","print(\"  - Step 1-4 complete analysis (today's data)\")\n","print(\"  - Gini Index comparison chart\")\n","print(\"  - Runtime performance comparison\")\n","print(\"  - 4 interactive switchable maps\")\n","print(\"  - Feature importance analysis\")\n","print(\"  - Comprehensive conclusions\")\n","print(\"\\nDownloading...\")\n","\n","try:\n","    from google.colab import files\n","    files.download(output_file)\n","    print(\"Dashboard downloaded successfully!\")\n","except ImportError:\n","    print(\"Not in Colab - file saved locally\")\n","except Exception as e:\n","    print(f\"Download failed: {e}\")\n","    print(\"Please run: files.download('health_risk_analysis_dashboard_COMPLETE.html')\")\n","\n","print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":816},"id":"dDBRuFcBqz8V","executionInfo":{"status":"ok","timestamp":1763838268864,"user_tz":480,"elapsed":16395,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"c7c0dfb3-85b3-416d-a10c-a18359d2a483"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","GENERATING COMPLETE DASHBOARD WITH LATEST DATA\n","======================================================================\n","\n","[1/8] Loading all result files...\n","  All files loaded successfully!\n","\n","[2/8] Downloading GeoJSON...\n","  GeoJSON downloaded\n","\n","[3/8] Creating Gini Index chart...\n","\n","[4/8] Creating runtime chart...\n","  Timing columns: ['Model', 'Training_Time_Seconds']\n","\n","[5/8] Creating DBSCAN scatter plot...\n","  Warning: Could not create DBSCAN plot: 'Cluster'\n","\n","[6/8] Creating interactive maps...\n","  4 interactive maps created\n","\n","[6/8] Calculating key metrics...\n","\n","[7/8] Building HTML dashboard...\n","\n","[8/8] Saving dashboard...\n","\n","======================================================================\n","COMPLETE DASHBOARD GENERATED!\n","======================================================================\n","\n","Output file: health_risk_analysis_dashboard_COMPLETE.html\n","File size: 20640.9 KB\n","\n","Dashboard includes:\n","  - Executive Summary with key metrics\n","  - Step 1-4 complete analysis (today's data)\n","  - Gini Index comparison chart\n","  - Runtime performance comparison\n","  - 4 interactive switchable maps\n","  - Feature importance analysis\n","  - Comprehensive conclusions\n","\n","Downloading...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_e850963e-a64f-4947-a4ee-78e74d4355f8\", \"health_risk_analysis_dashboard_COMPLETE.html\", 21136429)"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dashboard downloaded successfully!\n","======================================================================\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"jieheng.ic@gmail.com\"\n","!git config --global user.name \"Jie Heng\""],"metadata":{"id":"8xkDFG9Sf3sM","executionInfo":{"status":"ok","timestamp":1763864223044,"user_tz":480,"elapsed":134,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/IamSavitha/MLCountyHealth.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RFR8wZ_bgSZD","executionInfo":{"status":"ok","timestamp":1763864295794,"user_tz":480,"elapsed":2750,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"6743d2fb-4983-430d-9a9a-626d1fe2364d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'MLCountyHealth'...\n","remote: Enumerating objects: 144, done.\u001b[K\n","remote: Counting objects: 100% (144/144), done.\u001b[K\n","remote: Compressing objects: 100% (110/110), done.\u001b[K\n","remote: Total 144 (delta 40), reused 132 (delta 30), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (144/144), 26.67 MiB | 20.71 MiB/s, done.\n","Resolving deltas: 100% (40/40), done.\n"]}]},{"cell_type":"code","source":["!ls -la /content/MLCountyHealth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d68Pe_nugfQc","executionInfo":{"status":"ok","timestamp":1763864345708,"user_tz":480,"elapsed":112,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"b8f934c2-5c11-4b60-9f0b-d552c2390589"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["total 52\n","drwxr-xr-x 10 root root 4096 Nov 23 02:18 .\n","drwxr-xr-x  1 root root 4096 Nov 23 02:18 ..\n","drwxr-xr-x  3 root root 4096 Nov 23 02:18 archive\n","drwxr-xr-x  3 root root 4096 Nov 23 02:18 dashboards\n","drwxr-xr-x  5 root root 4096 Nov 23 02:18 data\n","drwxr-xr-x  4 root root 4096 Nov 23 02:18 docs\n","drwxr-xr-x  8 root root 4096 Nov 23 02:18 .git\n","-rw-r--r--  1 root root  309 Nov 23 02:18 .gitignore\n","drwxr-xr-x  2 root root 4096 Nov 23 02:18 models\n","drwxr-xr-x  2 root root 4096 Nov 23 02:18 notebooks\n","-rw-r--r--  1 root root 2594 Nov 23 02:18 README.md\n","-rw-r--r--  1 root root  284 Nov 23 02:18 requirements.txt\n","drwxr-xr-x  4 root root 4096 Nov 23 02:18 src\n"]}]},{"cell_type":"code","source":["!ls -la /content/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FpEIhR6ngyHO","executionInfo":{"status":"ok","timestamp":1763864422973,"user_tz":480,"elapsed":117,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"7d9ce3be-423e-4a9b-a289-91b895027370"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["total 20\n","drwxr-xr-x  1 root root 4096 Nov 23 02:18 .\n","drwxr-xr-x  1 root root 4096 Nov 23 02:16 ..\n","drwxr-xr-x  4 root root 4096 Nov 20 14:30 .config\n","drwxr-xr-x 10 root root 4096 Nov 23 02:18 MLCountyHealth\n","drwxr-xr-x  1 root root 4096 Nov 20 14:30 sample_data\n"]}]},{"cell_type":"code","source":["import os\n","print(\"directory\", os.getcwd())\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SyoARVYtg-zO","executionInfo":{"status":"ok","timestamp":1763864507474,"user_tz":480,"elapsed":17537,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"e72eecf3-ce1a-4d89-b928-2db90a89e451"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["directory /content\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["print(\"=== Drivenotebook ===\")\n","!find /content/drive/MyDrive -name \"*.ipynb\" -type f 2>/dev/null | grep -v \".Trash\" | head -20\n","\n","print(\"\\n=== 245notebook ===\")\n","!find /content/drive/MyDrive -name \"*245*.ipynb\" 2>/dev/null"],"metadata":{"id":"T4-wq01GhOGy","executionInfo":{"status":"ok","timestamp":1763864545054,"user_tz":480,"elapsed":973,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"258cf2f4-3e36-4fa0-d2ec-0cd1a46d0726","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["=== Drivenotebook ===\n","/content/drive/MyDrive/Colab Notebooks/Untitled0.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Assignment_JaneHeng.ipynb\n","/content/drive/MyDrive/Colab Notebooks/228-quiz-JaneHeng.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled1.ipynb\n","/content/drive/MyDrive/Colab Notebooks/shakespeare_rag_llamaindex_retrieval_only.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled2.ipynb\n","/content/drive/MyDrive/Colab Notebooks/245-1.3hw-group3.ipynb\n","/content/drive/MyDrive/Colab Notebooks/288project-EDA.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Electronic288project-EDA.ipynb\n","/content/drive/MyDrive/Colab Notebooks/SVM_source_classifier.ipynb\n","/content/drive/MyDrive/Colab Notebooks/test_source_classifier (1).ipynb\n","/content/drive/MyDrive/Colab Notebooks/245-2.3-JaneHeng.ipynb\n","/content/drive/MyDrive/Colab Notebooks/svm_source_classifier.ipynb\n","/content/drive/MyDrive/Colab Notebooks/test_source_classifier.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled3.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled4.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Untitled5.ipynb\n","/content/drive/MyDrive/Colab Notebooks/228-quiz4-JaneHeng.ipynb\n","/content/drive/MyDrive/Colab Notebooks/quiz5-Q1-JaneHeng.ipynb\n","/content/drive/MyDrive/Colab Notebooks/228hw4_JaneHeng.ipynb\n","\n","=== 245notebook ===\n","/content/drive/MyDrive/Colab Notebooks/245-1.3hw-group3.ipynb\n","/content/drive/MyDrive/Colab Notebooks/245-2.3-JaneHeng.ipynb\n","/content/drive/MyDrive/Colab Notebooks/245-project-janeheng.ipynb\n"]}]},{"cell_type":"code","source":["!cp /content/245-project-janeheng.ipynb /content/MLCountyHealth/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XwU2VfXYgkeN","executionInfo":{"status":"ok","timestamp":1763864401250,"user_tz":480,"elapsed":122,"user":{"displayName":"Jie Heng","userId":"06503907218628274525"}},"outputId":"5439a411-4119-4857-9e9d-321d17106013"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/245-project-janeheng.ipynb': No such file or directory\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/your-username/your-repo-name.git"],"metadata":{"id":"x5XFCq4tgE1n"},"execution_count":null,"outputs":[]}]}