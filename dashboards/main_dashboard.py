"""
Food Desert Effect on County-Level Health Outcomes - Final Dashboard
DATA-245 Machine Learning | Group 3

A streamlined ML analysis dashboard covering:
- Data Overview & EDA
- Regression Models (Linear, Ridge)
- Classification Models (Logistic Regression, SVM, Random Forest, Extra Trees)
- Advanced Ensemble Methods (Jane Heng's optimization pipeline)
- Clustering (K-Means)
- Dimensionality Reduction (PCA)
- Model Comparisons & Evaluations
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    mean_squared_error, r2_score, mean_absolute_error,
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, auc,
    silhouette_score
)

# Regression
from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression

# Classification
from sklearn.svm import SVC
from sklearn.ensemble import (
    RandomForestClassifier, ExtraTreesClassifier,
    GradientBoostingClassifier, BaggingClassifier,
    VotingClassifier, StackingClassifier
)

# Clustering
from sklearn.cluster import KMeans, DBSCAN
from sklearn.decomposition import PCA

from scipy.cluster.hierarchy import dendrogram, linkage

# Imbalanced learning
from imblearn.over_sampling import SMOTE

import warnings
warnings.filterwarnings('ignore')

# Page config
st.set_page_config(
    page_title="Food Desert ML Analysis",
    page_icon="üè•",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 36px;
        font-weight: bold;
        color: #1e88e5;
        text-align: center;
        padding: 15px;
        background: linear-gradient(90deg, #e3f2fd 0%, #90caf9 100%);
        border-radius: 10px;
        margin-bottom: 20px;
    }
    .metric-card {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 8px;
        border-left: 4px solid #1e88e5;
        margin: 5px 0;
    }
    .insight-box {
        background-color: #e8f5e9;
        padding: 12px;
        border-radius: 8px;
        border-left: 4px solid #4caf50;
        margin: 8px 0;
    }
    .warning-box {
        background-color: #fff3e0;
        padding: 12px;
        border-radius: 8px;
        border-left: 4px solid #ff9800;
        margin: 8px 0;
    }
</style>
""", unsafe_allow_html=True)

# Load data
@st.cache_data
def load_data():
    df = pd.read_csv('../data/processed/cleaned_health_data.csv')
    return df

df = load_data()

# Sidebar navigation
st.sidebar.markdown("## Navigation")
page = st.sidebar.radio(
    "Select Analysis",
    ["1. Project Overview",
     "2. Data Exploration",
     "3. Key Risk Factors",
     "4. Regression Analysis",
     "5. Classification Models",
     "5b. Advanced Ensemble Methods",
     "6. Clustering Analysis",
     "7. PCA Analysis",
     "8. Model Comparison",
     "9. Key Insights & Conclusions"]
)

# Feature columns for modeling
FEATURE_COLS = [
    'Food Environment Index', '% Children in Poverty',
    'Income Ratio', '% Uninsured', '% Completed High School',
    '% Rural', '% Excessive Drinking', '% Insufficient Sleep'
]

TARGET_REGRESSION = '% Adults with Obesity'
TARGET_CLASSIFICATION = 'High_Income_Inequality'

# =============================================================================
# PAGE 1: Project Overview
# =============================================================================
if page == "1. Project Overview":
    st.markdown('<div class="main-header">Food Desert Effect on County-Level Health Outcomes</div>', unsafe_allow_html=True)

    col1, col2 = st.columns([2, 1])

    with col1:
        st.markdown("### Project Objective")
        st.markdown("""
        Investigate how **food access inequality** correlates with community-level health outcomes
        (obesity, diabetes) across US counties using multiple machine learning approaches.
        """)

        st.markdown("### Research Questions")
        st.markdown("""
        1. What factors best predict obesity and diabetes rates at the county level?
        2. Can we identify distinct county profiles based on health and socioeconomic indicators?
        3. Which ML algorithms perform best for health outcome prediction?
        """)

        st.markdown("### ML Algorithms Covered")

        algo_col1, algo_col2, algo_col3 = st.columns(3)
        with algo_col1:
            st.markdown("**Regression**")
            st.markdown("- Linear Regression\n- Ridge (L2 Regularization)")
        with algo_col2:
            st.markdown("**Classification**")
            st.markdown("- Logistic Regression\n- SVM (RBF Kernel)\n- Random Forest\n- Extra Trees\n- Advanced Ensembles")
        with algo_col3:
            st.markdown("**Unsupervised**")
            st.markdown("- K-Means Clustering\n- PCA")

    with col2:
        st.markdown("### Dataset Summary")
        st.metric("Counties", f"{len(df):,}")
        st.metric("States", df['State'].nunique())
        st.metric("Features", len(df.columns))

        st.markdown("### Team")
        st.markdown("""
        - Savitha Vijayarangan (Lead & Data Integration Specialist)
        - Jane Heng (Statistical Analyst & Regression Lead )
        - Rishi Boppana (Clustering & PCA Specialist)
        - Kapil Sanikommu (Predictive Modeling & Visualization Lead)
        """)

# =============================================================================
# PAGE 2: Data Exploration
# =============================================================================
elif page == "2. Data Exploration":
    st.markdown('<div class="main-header">Data Exploration & EDA</div>', unsafe_allow_html=True)

    tab1, tab2, tab3 = st.tabs(["Dataset Overview", "Distributions", "Correlations"])

    with tab1:
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Total Counties", f"{len(df):,}")
        col2.metric("Avg Obesity Rate", f"{df['% Adults with Obesity'].mean():.1f}%")
        col3.metric("Avg Diabetes Rate", f"{df['% Adults with Diabetes'].mean():.1f}%")
        col4.metric("Avg Food Env Index", f"{df['Food Environment Index'].mean():.2f}")

        st.markdown("### Sample Data")
        st.dataframe(df.head(10), use_container_width=True)

        st.markdown("### Geographic Distribution")
        area_counts = df['Area_Type'].value_counts()
        fig = px.pie(values=area_counts.values, names=area_counts.index,
                     title="County Distribution by Area Type")
        st.plotly_chart(fig, use_container_width=True)

    with tab2:
        st.markdown("### Key Variable Distributions")

        vars_to_plot = ['% Adults with Obesity', '% Adults with Diabetes',
                        'Food Environment Index', '% Children in Poverty']

        fig = make_subplots(rows=2, cols=2, subplot_titles=vars_to_plot)

        for i, var in enumerate(vars_to_plot):
            row = i // 2 + 1
            col = i % 2 + 1
            fig.add_trace(
                go.Histogram(x=df[var], name=var, nbinsx=30),
                row=row, col=col
            )

        fig.update_layout(height=500, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)

        st.markdown("### Statistics Summary")
        st.dataframe(df[vars_to_plot].describe().round(2), use_container_width=True)

    with tab3:
        st.markdown("### Correlation Heatmap")

        corr_cols = ['% Adults with Obesity', '% Adults with Diabetes',
                     'Food Environment Index', '% Children in Poverty',
                     'Income Ratio', '% Uninsured', '% Rural']

        corr_matrix = df[corr_cols].corr()

        fig = px.imshow(corr_matrix,
                        labels=dict(color="Correlation"),
                        x=corr_cols, y=corr_cols,
                        color_continuous_scale='RdBu_r',
                        aspect='auto')
        fig.update_layout(height=500)
        st.plotly_chart(fig, use_container_width=True)

        st.markdown('<div class="insight-box"><strong>Key Insight:</strong> Food Environment Index shows strong negative correlation with obesity/diabetes rates, confirming food access impacts health.</div>', unsafe_allow_html=True)

# =============================================================================
# PAGE 3: Key Risk Factors
# =============================================================================
elif page == "3. Key Risk Factors":
    st.markdown('<div class="main-header">Key Risk Factors: Numbers That Matter</div>', unsafe_allow_html=True)

    st.markdown("### What drives obesity and diabetes at the county level?")

    # Calculate correlations
    features_analysis = ['Food Environment Index', '% Children in Poverty', 'Income Ratio',
                         '% Uninsured', '% Completed High School', '% Rural',
                         '% Excessive Drinking', '% Insufficient Sleep']

    tab1, tab2, tab3 = st.tabs(["Correlation Analysis", "Quantitative Impact", "High vs Low Counties"])

    with tab1:
        st.markdown("### Correlation Coefficients")
        st.markdown("*How strongly each factor relates to health outcomes (r = -1 to +1)*")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### Obesity Correlations")
            obesity_corrs = []
            for f in features_analysis:
                r = df[f].corr(df['% Adults with Obesity'])
                obesity_corrs.append({'Factor': f, 'Correlation (r)': r})

            ob_corr_df = pd.DataFrame(obesity_corrs).sort_values('Correlation (r)', key=abs, ascending=False)

            # Color code
            fig = px.bar(ob_corr_df, x='Correlation (r)', y='Factor', orientation='h',
                        color='Correlation (r)', color_continuous_scale='RdBu_r',
                        range_color=[-0.8, 0.8])
            fig.update_layout(height=400, showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

            st.markdown("""
            <div class="insight-box">
            <strong>Top 3 Obesity Drivers:</strong><br>
            1. % Insufficient Sleep (r = +0.52)<br>
            2. % Children in Poverty (r = +0.42)<br>
            3. % Completed High School (r = -0.42)
            </div>
            """, unsafe_allow_html=True)

        with col2:
            st.markdown("#### Diabetes Correlations")
            diabetes_corrs = []
            for f in features_analysis:
                r = df[f].corr(df['% Adults with Diabetes'])
                diabetes_corrs.append({'Factor': f, 'Correlation (r)': r})

            di_corr_df = pd.DataFrame(diabetes_corrs).sort_values('Correlation (r)', key=abs, ascending=False)

            fig = px.bar(di_corr_df, x='Correlation (r)', y='Factor', orientation='h',
                        color='Correlation (r)', color_continuous_scale='RdBu_r',
                        range_color=[-0.8, 0.8])
            fig.update_layout(height=400, showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

            st.markdown("""
            <div class="insight-box">
            <strong>Top 3 Diabetes Drivers:</strong><br>
            1. % Insufficient Sleep (r = +0.77)<br>
            2. % Completed High School (r = -0.74)<br>
            3. % Children in Poverty (r = +0.73)
            </div>
            """, unsafe_allow_html=True)

    with tab2:
        st.markdown("### Practical Impact: What the Numbers Mean")

        # Prepare regression for practical coefficients
        X_analysis = df[features_analysis].dropna()
        y_obesity = df.loc[X_analysis.index, '% Adults with Obesity']
        y_diabetes = df.loc[X_analysis.index, '% Adults with Diabetes']

        from sklearn.linear_model import LinearRegression

        model_ob = LinearRegression().fit(X_analysis, y_obesity)
        model_di = LinearRegression().fit(X_analysis, y_diabetes)

        st.markdown("#### Per-Unit Impact on Obesity Rate")

        impact_data = []
        for i, f in enumerate(features_analysis):
            impact_data.append({
                'Factor': f,
                'Obesity Impact': model_ob.coef_[i],
                'Diabetes Impact': model_di.coef_[i]
            })

        impact_df = pd.DataFrame(impact_data)

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("**Obesity: % change per unit increase**")
            ob_impact = impact_df[['Factor', 'Obesity Impact']].sort_values('Obesity Impact', key=abs, ascending=False)

            for _, row in ob_impact.iterrows():
                val = row['Obesity Impact']
                direction = "+" if val > 0 else ""
                st.markdown(f"- **{row['Factor']}**: {direction}{val:.3f}%")

        with col2:
            st.markdown("**Diabetes: % change per unit increase**")
            di_impact = impact_df[['Factor', 'Diabetes Impact']].sort_values('Diabetes Impact', key=abs, ascending=False)

            for _, row in di_impact.iterrows():
                val = row['Diabetes Impact']
                direction = "+" if val > 0 else ""
                st.markdown(f"- **{row['Factor']}**: {direction}{val:.3f}%")

        st.markdown("---")

        st.markdown("### Key Interpretations")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown("""
            <div class="metric-card">
            <h4>Sleep Deprivation</h4>
            <p style="font-size: 24px; font-weight: bold; color: #d32f2f;">+0.33%</p>
            <p>obesity per 1% more sleep-deprived adults</p>
            </div>
            """, unsafe_allow_html=True)

        with col2:
            st.markdown("""
            <div class="metric-card">
            <h4>Food Environment</h4>
            <p style="font-size: 24px; font-weight: bold; color: #388e3c;">-0.08%</p>
            <p>obesity per 1-point better food index</p>
            </div>
            """, unsafe_allow_html=True)

        with col3:
            st.markdown("""
            <div class="metric-card">
            <h4>Child Poverty</h4>
            <p style="font-size: 24px; font-weight: bold; color: #d32f2f;">+0.07%</p>
            <p>obesity per 1% more children in poverty</p>
            </div>
            """, unsafe_allow_html=True)

        st.markdown("""
        <div class="warning-box">
        <strong>Why Food Environment appears "small":</strong><br>
        The Food Environment Index ranges from 0-10 (narrow scale), while other factors range 0-100%.
        A 1-point improvement in food index (e.g., 6‚Üí7) is actually a <strong>significant intervention</strong>.<br><br>
        <strong>Scaled comparison:</strong> Improving food environment by 5 points ‚Üí -0.42% obesity (comparable to reducing poverty by 6%)
        </div>
        """, unsafe_allow_html=True)

    with tab3:
        st.markdown("### High vs Low Counties Comparison")
        st.markdown("*Comparing top 25% vs bottom 25% of counties by health outcomes*")

        # Calculate quartile comparisons
        for target, target_name in [('% Adults with Obesity', 'Obesity'), ('% Adults with Diabetes', 'Diabetes')]:
            st.markdown(f"#### {target_name} Comparison")

            q25 = df[target].quantile(0.25)
            q75 = df[target].quantile(0.75)
            low = df[df[target] <= q25]
            high = df[df[target] >= q75]

            comparison_data = []
            key_factors = ['Food Environment Index', '% Children in Poverty', '% Insufficient Sleep',
                          '% Completed High School', 'Income Ratio']

            for f in key_factors:
                low_mean = low[f].mean()
                high_mean = high[f].mean()
                diff = high_mean - low_mean
                pct_diff = (diff / low_mean * 100) if low_mean != 0 else 0
                comparison_data.append({
                    'Factor': f,
                    f'Low {target_name} Counties': round(low_mean, 1),
                    f'High {target_name} Counties': round(high_mean, 1),
                    'Difference': round(diff, 1),
                    '% Difference': round(pct_diff, 1)
                })

            comp_df = pd.DataFrame(comparison_data)
            st.dataframe(comp_df, use_container_width=True)

            # Visual comparison
            fig = go.Figure()
            fig.add_trace(go.Bar(name=f'Low {target_name}', x=key_factors,
                                y=[low[f].mean() for f in key_factors]))
            fig.add_trace(go.Bar(name=f'High {target_name}', x=key_factors,
                                y=[high[f].mean() for f in key_factors]))
            fig.update_layout(barmode='group', height=350, title=f'{target_name}: Low vs High Counties')
            st.plotly_chart(fig, use_container_width=True)

        st.markdown("""
        <div class="insight-box">
        <strong>Key Findings:</strong><br>
        ‚Ä¢ High-diabetes counties have <strong>12% more child poverty</strong> than low-diabetes counties<br>
        ‚Ä¢ High-obesity counties have <strong>5% more sleep-deprived</strong> residents<br>
        ‚Ä¢ Food Environment Index is <strong>1.4 points lower</strong> in high-diabetes vs low-diabetes counties<br>
        ‚Ä¢ Education gap: High-disease counties have <strong>8-10% lower</strong> high school completion
        </div>
        """, unsafe_allow_html=True)

# =============================================================================
# PAGE 4: Regression Analysis
# =============================================================================
elif page == "4. Regression Analysis":
    st.markdown('<div class="main-header">Regression Analysis</div>', unsafe_allow_html=True)

    st.markdown("**Target:** % Adults with Obesity | **Goal:** Predict continuous obesity rates from socioeconomic factors")

    # Prepare data
    X = df[FEATURE_COLS].dropna()
    y = df.loc[X.index, TARGET_REGRESSION]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Train models
    models = {
        'Linear Regression': LinearRegression(),
        'Ridge (Œ±=1.0)': Ridge(alpha=1.0)
    }

    results = []
    predictions = {}
    coefficients = {}

    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        predictions[name] = y_pred

        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')

        results.append({
            'Model': name,
            'R¬≤': r2,
            'RMSE': rmse,
            'MAE': mae,
            'CV R¬≤ (mean)': cv_scores.mean(),
            'CV R¬≤ (std)': cv_scores.std()
        })

        coefficients[name] = pd.DataFrame({
            'Feature': FEATURE_COLS,
            'Coefficient': model.coef_
        }).sort_values('Coefficient', key=abs, ascending=False)

    # Display results
    tab1, tab2, tab3, tab4 = st.tabs(["Algorithm Details", "Model Comparison", "Predictions & Residuals", "Feature Importance"])

    with tab1:
        st.markdown("### Algorithm Overview")

        st.markdown("""
        We compare two regression approaches to predict obesity and diabetes rates:
        - **Linear Regression (OLS)**: Baseline model with no regularization
        - **Ridge Regression (L2)**: Regularized model that handles multicollinearity

        **Why these two?**
        - Linear provides interpretable baseline performance
        - Ridge adds regularization to address correlated features (e.g., poverty ‚Üî education)
        - Lasso was removed because it retained all 7 features (no sparsity benefit) and performed nearly identically to Ridge
        """)

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            <div class="metric-card">
            <h4>Linear Regression (OLS)</h4>
            <p><strong>Goal:</strong> Minimize sum of squared residuals</p>
            <p><strong>Equation:</strong> y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚Çô</p>
            <p><strong>Parameters:</strong> Coefficients (Œ≤)</p>
            <p><strong>Hyperparameters:</strong> None</p>
            <p><strong>Pros:</strong> Interpretable, fast, baseline</p>
            <p><strong>Cons:</strong> Sensitive to multicollinearity</p>
            </div>
            """, unsafe_allow_html=True)

        with col2:
            st.markdown("""
            <div class="metric-card">
            <h4>Ridge Regression (L2)</h4>
            <p><strong>Goal:</strong> OLS + penalty on large coefficients</p>
            <p><strong>Equation:</strong> Loss = RSS + Œ±‚àëŒ≤¬≤</p>
            <p><strong>Parameters:</strong> Coefficients (Œ≤)</p>
            <p><strong>Hyperparameters:</strong> Œ± (regularization strength)</p>
            <p><strong>Pros:</strong> Handles multicollinearity, stable</p>
            <p><strong>Cons:</strong> Retains all features (no sparsity)</p>
            <p><strong>Result:</strong> 1.4% improvement over linear (R¬≤ = 0.417)</p>
            </div>
            """, unsafe_allow_html=True)

        st.markdown("### Metric Interpretation")
        st.markdown("""
        | Metric | Formula | Interpretation |
        |--------|---------|----------------|
        | **R¬≤** | 1 - (SS_res/SS_tot) | % variance explained (0-1, higher is better) |
        | **RMSE** | ‚àö(Œ£(y-≈∑)¬≤/n) | Average prediction error in same units as target |
        | **MAE** | Œ£\|y-≈∑\|/n | Average absolute error (less sensitive to outliers) |
        | **CV Score** | Mean R¬≤ across folds | Generalization performance |
        """)

    with tab2:
        st.markdown("### Model Performance Comparison")
        results_df = pd.DataFrame(results)

        # Visual comparison
        fig = go.Figure()
        metrics = ['R¬≤', 'CV R¬≤ (mean)']
        for metric in metrics:
            fig.add_trace(go.Bar(name=metric, x=results_df['Model'], y=results_df[metric]))
        fig.update_layout(barmode='group', title='R¬≤ Comparison (Higher is Better)',
                         yaxis_title='Score', height=350)
        st.plotly_chart(fig, use_container_width=True)

        col1, col2, col3 = st.columns(3)
        for i, row in results_df.iterrows():
            with [col1, col2, col3][i]:
                st.markdown(f"### {row['Model']}")
                st.metric("R¬≤", f"{row['R¬≤']:.4f}")
                st.metric("RMSE", f"{row['RMSE']:.3f}%")
                st.metric("MAE", f"{row['MAE']:.3f}%")
                st.metric("CV R¬≤", f"{row['CV R¬≤ (mean)']:.4f} ¬± {row['CV R¬≤ (std)']:.4f}")

        st.markdown("### Results Table")
        st.dataframe(results_df.round(4), use_container_width=True)

        # Interpretation
        best_model = results_df.loc[results_df['R¬≤'].idxmax(), 'Model']
        best_r2 = results_df['R¬≤'].max()
        st.markdown(f"""
        <div class="insight-box">
        <strong>Interpretation:</strong><br>
        ‚Ä¢ <strong>{best_model}</strong> achieves R¬≤ = {best_r2:.4f}, explaining <strong>{best_r2*100:.1f}%</strong> of variance in obesity rates<br>
        ‚Ä¢ RMSE of ~{results_df['RMSE'].mean():.2f}% means predictions are off by about {results_df['RMSE'].mean():.1f} percentage points on average<br>
        ‚Ä¢ Similar performance across models suggests stable, well-behaved data
        </div>
        """, unsafe_allow_html=True)

    with tab3:
        st.markdown("### Actual vs Predicted & Residual Analysis")
        model_choice = st.selectbox("Select Model", list(models.keys()))

        col1, col2 = st.columns(2)

        with col1:
            # Actual vs Predicted
            fig = px.scatter(x=y_test, y=predictions[model_choice],
                             labels={'x': 'Actual Obesity %', 'y': 'Predicted Obesity %'},
                             title=f'{model_choice}: Actual vs Predicted')
            fig.add_trace(go.Scatter(x=[y_test.min(), y_test.max()],
                                      y=[y_test.min(), y_test.max()],
                                      mode='lines', name='Perfect Fit',
                                      line=dict(dash='dash', color='red')))
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)

            st.markdown("""
            <div class="insight-box">
            <strong>Chart Interpretation:</strong><br>
            Points close to red line = accurate predictions.<br>
            Scatter = prediction uncertainty.
            </div>
            """, unsafe_allow_html=True)

        with col2:
            # Residuals
            residuals = y_test - predictions[model_choice]
            fig2 = px.histogram(residuals, nbins=30, title='Residual Distribution',
                               labels={'value': 'Residual (Actual - Predicted)'})
            fig2.add_vline(x=0, line_dash='dash', line_color='red')
            fig2.update_layout(height=400)
            st.plotly_chart(fig2, use_container_width=True)

            st.markdown(f"""
            <div class="insight-box">
            <strong>Residual Stats:</strong><br>
            Mean: {residuals.mean():.3f} (should be ~0)<br>
            Std: {residuals.std():.3f}<br>
            Normal distribution = good model fit
            </div>
            """, unsafe_allow_html=True)

        # Residual vs Predicted (homoscedasticity check)
        fig3 = px.scatter(x=predictions[model_choice], y=residuals,
                         labels={'x': 'Predicted', 'y': 'Residual'},
                         title='Residuals vs Predicted (Homoscedasticity Check)')
        fig3.add_hline(y=0, line_dash='dash', line_color='red')
        st.plotly_chart(fig3, use_container_width=True)

        st.markdown("""
        <div class="warning-box">
        <strong>What to look for:</strong> Random scatter around 0 = good. Funnel shape = heteroscedasticity problem.
        </div>
        """, unsafe_allow_html=True)

    with tab4:
        st.markdown("### Feature Coefficients & Importance")
        model_choice = st.selectbox("Select Model for Coefficients", list(models.keys()), key='coef')

        coef_df = coefficients[model_choice]

        # Coefficient plot
        fig = px.bar(coef_df, x='Coefficient', y='Feature', orientation='h',
                     title=f'{model_choice} - Standardized Coefficients',
                     color='Coefficient', color_continuous_scale='RdBu_r')
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)

        # Interpretation table
        st.markdown("### Coefficient Interpretation")
        interp_data = []
        for _, row in coef_df.iterrows():
            direction = "increases" if row['Coefficient'] > 0 else "decreases"
            interp_data.append({
                'Feature': row['Feature'],
                'Coefficient': f"{row['Coefficient']:+.3f}",
                'Interpretation': f"1 SD increase ‚Üí obesity {direction} by {abs(row['Coefficient']):.2f}%"
            })
        st.dataframe(pd.DataFrame(interp_data), use_container_width=True)

        st.markdown("""
        <div class="insight-box">
        <strong>Key Findings:</strong><br>
        ‚Ä¢ <strong>% Insufficient Sleep</strong> (+1.24): Strongest positive predictor - sleep deprivation strongly linked to obesity<br>
        ‚Ä¢ <strong>% Completed High School</strong> (-0.49): Education is protective<br>
        ‚Ä¢ <strong>% Children in Poverty</strong> (+0.46): Poverty increases obesity risk<br>
        ‚Ä¢ <strong>Food Environment Index</strong> (-0.08): Better food access reduces obesity (small but significant)
        </div>
        """, unsafe_allow_html=True)

# =============================================================================
# PAGE 5: Classification Models
# =============================================================================
elif page == "5. Classification Models":
    st.markdown('<div class="main-header">Classification Models</div>', unsafe_allow_html=True)

    st.markdown("**Target:** High Income Inequality (binary) | **Goal:** Classify counties into high/low income inequality groups")

    # Prepare data
    X = df[FEATURE_COLS].dropna()
    y = df.loc[X.index, TARGET_CLASSIFICATION]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Apply SMOTE
    smote = SMOTE(random_state=42)
    X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

    # Define models
    models = {
        'Logistic Regression': LogisticRegression(max_iter=1000),
        'SVM (RBF)': SVC(kernel='rbf', probability=True),
        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),
        'Extra Trees': ExtraTreesClassifier(n_estimators=100, max_depth=5, random_state=42)
    }

    results = []
    predictions = {}
    probabilities = {}

    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        predictions[name] = y_pred

        if hasattr(model, 'predict_proba'):
            probabilities[name] = model.predict_proba(X_test_scaled)[:, 1]

        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred)
        rec = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')

        results.append({
            'Model': name,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1 Score': f1,
            'CV Accuracy': cv_scores.mean()
        })

    results_df = pd.DataFrame(results).sort_values('F1 Score', ascending=False)

    # Train models with SMOTE
    smote_results = []
    smote_predictions = {}
    smote_probabilities = {}

    for name, model in models.items():
        # Create fresh model instance for SMOTE
        if name == 'Logistic Regression':
            smote_model = LogisticRegression(max_iter=1000)
        elif name == 'SVM (RBF)':
            smote_model = SVC(kernel='rbf', probability=True)
        elif name == 'Random Forest':
            smote_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
        else:  # Extra Trees
            smote_model = ExtraTreesClassifier(n_estimators=100, max_depth=5, random_state=42)

        smote_model.fit(X_train_smote, y_train_smote)
        y_pred = smote_model.predict(X_test_scaled)
        smote_predictions[name] = y_pred

        if hasattr(smote_model, 'predict_proba'):
            smote_probabilities[name] = smote_model.predict_proba(X_test_scaled)[:, 1]

        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred)
        rec = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)

        smote_results.append({
            'Model': name,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1 Score': f1
        })

    smote_results_df = pd.DataFrame(smote_results).sort_values('F1 Score', ascending=False)

    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["Algorithm Details", "Performance Comparison", "Confusion Matrices", "ROC Curves", "Feature Importance", "SMOTE Comparison"])

    with tab1:
        st.markdown("### Algorithm Overview")

        st.markdown("""
        We selected 4 classification algorithms representing a progression from simple to sophisticated:
        - **Logistic Regression**: Linear baseline for interpretability
        - **SVM (RBF)**: Nonlinear kernel method for complex boundaries
        - **Random Forest & Extra Trees**: Top-performing ensemble methods

        **Removed algorithms and why:**
        - **KNN**: Weakest performer (F1 = 0.771), slow, no insights
        - **Naive Bayes**: Mediocre (F1 = 0.789), independence assumption violated (poverty ‚Üî education highly correlated)
        - **Decision Tree**: Redundant with Random Forest (which IS an ensemble of decision trees), poor performance (F1 = 0.788)
        """)

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.markdown("""
            <div class="metric-card">
            <h5>Logistic Regression</h5>
            <p><strong>Type:</strong> Linear classifier</p>
            <p><strong>Goal:</strong> Linear decision boundary via sigmoid</p>
            <p><strong>Hyperparams:</strong> C, max_iter</p>
            <p><strong>Pros:</strong> Interpretable coefficients, probabilistic outputs, fast</p>
            <p><strong>Cons:</strong> Assumes linear separability</p>
            <p><strong>Result:</strong> F1 = 0.815 (solid baseline)</p>
            </div>
            """, unsafe_allow_html=True)

        with col2:
            st.markdown("""
            <div class="metric-card">
            <h5>SVM (RBF Kernel)</h5>
            <p><strong>Type:</strong> Kernel method</p>
            <p><strong>Goal:</strong> Maximum margin with nonlinear boundaries</p>
            <p><strong>Hyperparams:</strong> C, kernel, gamma</p>
            <p><strong>Pros:</strong> Handles nonlinearity, effective in high dimensions</p>
            <p><strong>Cons:</strong> Slower training, less interpretable</p>
            <p><strong>Result:</strong> F1 = 0.809 (strong nonlinear)</p>
            </div>
            """, unsafe_allow_html=True)

        with col3:
            st.markdown("""
            <div class="metric-card">
            <h5>Random Forest</h5>
            <p><strong>Type:</strong> Ensemble (bagging)</p>
            <p><strong>Goal:</strong> Combine many decision trees</p>
            <p><strong>Hyperparams:</strong> n_estimators=100, max_depth=5</p>
            <p><strong>Pros:</strong> Best performance, feature importance, robust</p>
            <p><strong>Cons:</strong> Less interpretable than single tree</p>
            <p><strong>Result:</strong> F1 = 0.848 (BEST) ‚úì</p>
            </div>
            """, unsafe_allow_html=True)

        with col4:
            st.markdown("""
            <div class="metric-card">
            <h5>Extra Trees</h5>
            <p><strong>Type:</strong> Ensemble (extreme randomization)</p>
            <p><strong>Goal:</strong> Random splits for faster training</p>
            <p><strong>Hyperparams:</strong> n_estimators=100, max_depth=5</p>
            <p><strong>Pros:</strong> Faster than RF, competitive performance</p>
            <p><strong>Cons:</strong> Slightly higher bias</p>
            <p><strong>Result:</strong> F1 = 0.843 (close second)</p>
            </div>
            """, unsafe_allow_html=True)

        st.markdown("### Metric Interpretation")
        st.markdown("""
        | Metric | Formula | Interpretation |
        |--------|---------|----------------|
        | **Accuracy** | (TP+TN)/(TP+TN+FP+FN) | Overall correctness (misleading for imbalanced data) |
        | **Precision** | TP/(TP+FP) | Of predicted positives, how many are correct? |
        | **Recall** | TP/(TP+FN) | Of actual positives, how many did we find? |
        | **F1 Score** | 2√ó(Prec√óRec)/(Prec+Rec) | Harmonic mean of precision & recall |
        | **AUC** | Area under ROC | Discrimination ability (0.5=random, 1=perfect) |
        """)

    with tab2:
        st.markdown("### Model Performance Comparison")

        # Bar chart comparison
        fig = go.Figure()
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
        for metric in metrics:
            fig.add_trace(go.Bar(name=metric, x=results_df['Model'], y=results_df[metric]))

        fig.update_layout(barmode='group', height=400,
                          title='Classification Metrics Comparison')
        st.plotly_chart(fig, use_container_width=True)

        st.dataframe(results_df.round(4), use_container_width=True)

        best_model = results_df.iloc[0]['Model']
        best_f1 = results_df.iloc[0]['F1 Score']
        best_acc = results_df.iloc[0]['Accuracy']

        st.markdown(f"""
        <div class="insight-box">
        <strong>Interpretation:</strong><br>
        ‚Ä¢ <strong>{best_model}</strong> achieves best F1 = {best_f1:.4f}<br>
        ‚Ä¢ Accuracy of {best_acc:.1%} means {best_acc*100:.1f}% of counties correctly classified<br>
        ‚Ä¢ Ensemble methods (RF, Extra Trees) typically outperform single models<br>
        ‚Ä¢ Similar F1 and Accuracy suggests balanced class distribution
        </div>
        """, unsafe_allow_html=True)

    with tab3:
        st.markdown("### Confusion Matrix Analysis")
        model_choice = st.selectbox("Select Model", list(models.keys()))

        cm = confusion_matrix(y_test, predictions[model_choice])
        tn, fp, fn, tp = cm.ravel()

        col1, col2 = st.columns([2, 1])

        with col1:
            fig = px.imshow(cm, labels=dict(x="Predicted", y="Actual"),
                            x=['Low Inequality', 'High Inequality'],
                            y=['Low Inequality', 'High Inequality'],
                            text_auto=True, color_continuous_scale='Blues',
                            title=f'{model_choice} - Confusion Matrix')
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            st.markdown("### Matrix Breakdown")
            st.markdown(f"""
            <div class="metric-card">
            <p><strong>True Negatives:</strong> {tn}</p>
            <p>Correctly predicted Low Inequality</p>
            </div>
            """, unsafe_allow_html=True)

            st.markdown(f"""
            <div class="metric-card">
            <p><strong>True Positives:</strong> {tp}</p>
            <p>Correctly predicted High Inequality</p>
            </div>
            """, unsafe_allow_html=True)

            st.markdown(f"""
            <div class="warning-box">
            <p><strong>False Positives:</strong> {fp}</p>
            <p>Low incorrectly labeled High</p>
            <p><strong>False Negatives:</strong> {fn}</p>
            <p>High incorrectly labeled Low</p>
            </div>
            """, unsafe_allow_html=True)

    with tab4:
        st.markdown("### ROC Curves & AUC")

        fig = go.Figure()

        auc_scores = {}
        for name in models.keys():
            if name in probabilities:
                fpr, tpr, _ = roc_curve(y_test, probabilities[name])
                roc_auc = auc(fpr, tpr)
                auc_scores[name] = roc_auc
                fig.add_trace(go.Scatter(x=fpr, y=tpr, name=f'{name} (AUC={roc_auc:.3f})',
                                         mode='lines'))

        fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines',
                                  name='Random (AUC=0.5)', line=dict(dash='dash', color='gray')))

        fig.update_layout(title='ROC Curves Comparison',
                          xaxis_title='False Positive Rate (1 - Specificity)',
                          yaxis_title='True Positive Rate (Sensitivity)',
                          height=500)
        st.plotly_chart(fig, use_container_width=True)

        st.markdown("""
        <div class="insight-box">
        <strong>ROC Interpretation:</strong><br>
        ‚Ä¢ <strong>Closer to top-left</strong> = better model<br>
        ‚Ä¢ <strong>AUC > 0.8</strong> = good discrimination<br>
        ‚Ä¢ <strong>AUC = 0.5</strong> = random guessing (diagonal line)<br>
        ‚Ä¢ Curves above diagonal = better than random
        </div>
        """, unsafe_allow_html=True)

        if auc_scores:
            best_auc_model = max(auc_scores, key=auc_scores.get)
            st.markdown(f"**Best AUC:** {best_auc_model} with AUC = {auc_scores[best_auc_model]:.4f}")

    with tab5:
        st.markdown("### Feature Importance Analysis")

        # Get feature importance from tree-based models
        tree_models = {
            'Random Forest': models['Random Forest'],
            'Extra Trees': models['Extra Trees']
        }

        model_choice = st.selectbox("Select Tree-based Model", list(tree_models.keys()), key='feat_imp')

        # Feature importance
        importance = tree_models[model_choice].feature_importances_
        feat_imp_df = pd.DataFrame({
            'Feature': FEATURE_COLS,
            'Importance': importance
        }).sort_values('Importance', ascending=True)

        # Bar chart
        fig = px.bar(feat_imp_df, x='Importance', y='Feature', orientation='h',
                     title=f'{model_choice} - Feature Importance',
                     color='Importance', color_continuous_scale='Viridis')
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)

        # Importance table with percentages
        feat_imp_df['Importance %'] = (feat_imp_df['Importance'] / feat_imp_df['Importance'].sum() * 100).round(2)
        feat_imp_display = feat_imp_df.sort_values('Importance', ascending=False)[['Feature', 'Importance', 'Importance %']]
        st.dataframe(feat_imp_display, use_container_width=True)

        # Compare all tree models
        st.markdown("### Compare Feature Importance Across Models")

        fig2 = go.Figure()
        for name, model in tree_models.items():
            imp = model.feature_importances_
            fig2.add_trace(go.Bar(name=name, x=FEATURE_COLS, y=imp))

        fig2.update_layout(barmode='group', height=400,
                          title='Feature Importance Comparison',
                          xaxis_title='Feature', yaxis_title='Importance')
        st.plotly_chart(fig2, use_container_width=True)

        # Logistic Regression coefficients
        st.markdown("### Logistic Regression Coefficients")

        log_coef = models['Logistic Regression'].coef_[0]
        log_coef_df = pd.DataFrame({
            'Feature': FEATURE_COLS,
            'Coefficient': log_coef
        }).sort_values('Coefficient', key=abs, ascending=True)

        fig3 = px.bar(log_coef_df, x='Coefficient', y='Feature', orientation='h',
                     title='Logistic Regression - Coefficients (log-odds)',
                     color='Coefficient', color_continuous_scale='RdBu_r')
        fig3.update_layout(height=400)
        st.plotly_chart(fig3, use_container_width=True)

        st.markdown("""
        <div class="insight-box">
        <strong>Feature Importance Interpretation:</strong><br>
        ‚Ä¢ <strong>Tree-based models:</strong> Higher importance = feature used more often in splits<br>
        ‚Ä¢ <strong>Logistic Regression:</strong> Positive coefficient = increases probability of High Inequality<br>
        ‚Ä¢ <strong>Income Ratio</strong> and <strong>% Children in Poverty</strong> are top predictors of income inequality<br>
        ‚Ä¢ <strong>Food Environment</strong> and <strong>Education</strong> are protective factors (negative coefficients)
        </div>
        """, unsafe_allow_html=True)

    with tab6:
        st.markdown("### SMOTE (Synthetic Minority Over-sampling Technique)")

        # Class distribution comparison
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### Original Data Distribution")
            orig_dist = pd.Series(y_train).value_counts()
            fig_orig = px.pie(values=orig_dist.values, names=['Low Inequality', 'High Inequality'],
                             title='Original Training Data',
                             color_discrete_sequence=['#2196F3', '#FF5722'])
            fig_orig.update_layout(height=300)
            st.plotly_chart(fig_orig, use_container_width=True)
            st.metric("Class Ratio", f"{orig_dist.max()/orig_dist.min():.2f}:1")
            st.write(f"Total samples: {len(y_train)}")

        with col2:
            st.markdown("#### SMOTE Resampled Distribution")
            smote_dist = pd.Series(y_train_smote).value_counts()
            fig_smote = px.pie(values=smote_dist.values, names=['Low Inequality', 'High Inequality'],
                              title='SMOTE Training Data',
                              color_discrete_sequence=['#2196F3', '#FF5722'])
            fig_smote.update_layout(height=300)
            st.plotly_chart(fig_smote, use_container_width=True)
            st.metric("Class Ratio", "1:1 (balanced)")
            st.write(f"Total samples: {len(y_train_smote)}")

        st.markdown("---")
        st.markdown("### Performance Comparison: Original vs SMOTE")

        # Merge results for comparison
        comparison_df = results_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score']].copy()
        comparison_df = comparison_df.rename(columns={
            'Accuracy': 'Orig Accuracy', 'Precision': 'Orig Precision',
            'Recall': 'Orig Recall', 'F1 Score': 'Orig F1'
        })

        smote_compare = smote_results_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score']].copy()
        smote_compare = smote_compare.rename(columns={
            'Accuracy': 'SMOTE Accuracy', 'Precision': 'SMOTE Precision',
            'Recall': 'SMOTE Recall', 'F1 Score': 'SMOTE F1'
        })

        full_comparison = comparison_df.merge(smote_compare, on='Model')

        # Calculate differences
        full_comparison['Acc Œî'] = (full_comparison['SMOTE Accuracy'] - full_comparison['Orig Accuracy']) * 100
        full_comparison['Prec Œî'] = (full_comparison['SMOTE Precision'] - full_comparison['Orig Precision']) * 100
        full_comparison['Rec Œî'] = (full_comparison['SMOTE Recall'] - full_comparison['Orig Recall']) * 100
        full_comparison['F1 Œî'] = (full_comparison['SMOTE F1'] - full_comparison['Orig F1']) * 100

        # Display comparison table
        display_cols = ['Model', 'Orig F1', 'SMOTE F1', 'F1 Œî', 'Orig Recall', 'SMOTE Recall', 'Rec Œî']
        st.dataframe(
            full_comparison[display_cols].style.format({
                'Orig F1': '{:.3f}', 'SMOTE F1': '{:.3f}', 'F1 Œî': '{:+.1f}%',
                'Orig Recall': '{:.3f}', 'SMOTE Recall': '{:.3f}', 'Rec Œî': '{:+.1f}%'
            }).background_gradient(subset=['F1 Œî', 'Rec Œî'], cmap='RdYlGn'),
            use_container_width=True
        )

        # Bar chart comparison
        fig_compare = go.Figure()

        models_list = full_comparison['Model'].tolist()

        fig_compare.add_trace(go.Bar(
            name='Original',
            x=models_list,
            y=full_comparison['Orig F1'],
            marker_color='#2196F3'
        ))

        fig_compare.add_trace(go.Bar(
            name='SMOTE',
            x=models_list,
            y=full_comparison['SMOTE F1'],
            marker_color='#FF5722'
        ))

        fig_compare.update_layout(
            barmode='group',
            title='F1 Score Comparison: Original vs SMOTE',
            xaxis_title='Model',
            yaxis_title='F1 Score',
            height=400
        )
        st.plotly_chart(fig_compare, use_container_width=True)

        # Recall comparison (SMOTE typically improves recall)
        fig_recall = go.Figure()

        fig_recall.add_trace(go.Bar(
            name='Original',
            x=models_list,
            y=full_comparison['Orig Recall'],
            marker_color='#4CAF50'
        ))

        fig_recall.add_trace(go.Bar(
            name='SMOTE',
            x=models_list,
            y=full_comparison['SMOTE Recall'],
            marker_color='#9C27B0'
        ))

        fig_recall.update_layout(
            barmode='group',
            title='Recall Comparison: Original vs SMOTE',
            xaxis_title='Model',
            yaxis_title='Recall',
            height=400
        )
        st.plotly_chart(fig_recall, use_container_width=True)

        # Insights
        avg_f1_change = full_comparison['F1 Œî'].mean()
        avg_recall_change = full_comparison['Rec Œî'].mean()
        best_improvement = full_comparison.loc[full_comparison['F1 Œî'].idxmax(), 'Model']

        if avg_f1_change > 0:
            insight_color = "insight-box"
            insight_text = f"SMOTE improved average F1 by <strong>{avg_f1_change:.1f}%</strong>"
        else:
            insight_color = "warning-box"
            insight_text = f"SMOTE decreased average F1 by <strong>{abs(avg_f1_change):.1f}%</strong>"

        st.markdown(f"""
        <div class="{insight_color}">
        <strong>SMOTE Analysis Summary:</strong><br>
        ‚Ä¢ {insight_text}<br>
        ‚Ä¢ Average Recall change: <strong>{avg_recall_change:+.1f}%</strong><br>
        ‚Ä¢ Best improvement: <strong>{best_improvement}</strong><br>
        ‚Ä¢ <strong>Note:</strong> Since the original data was already balanced (1.13:1), SMOTE may not show dramatic improvements. SMOTE is most effective when class imbalance is severe (>3:1).
        </div>
        """, unsafe_allow_html=True)

        st.markdown("""
        <div class="metric-card">
        <strong>Understanding SMOTE:</strong><br>
        ‚Ä¢ Creates synthetic samples by interpolating between existing minority class samples<br>
        ‚Ä¢ Helps models learn better decision boundaries for minority class<br>
        ‚Ä¢ Trade-off: May increase false positives while improving true positive detection<br>
        ‚Ä¢ Best practice: Always evaluate on original (non-resampled) test data
        </div>
        """, unsafe_allow_html=True)

# =============================================================================
# PAGE 5B: Advanced Ensemble Methods (Jane Heng's Optimization Pipeline)
# =============================================================================
elif page == "5b. Advanced Ensemble Methods":
    st.markdown('<div class="main-header">Advanced Ensemble Methods: Model Optimization Pipeline</div>', unsafe_allow_html=True)

    st.markdown("""
    **Contributor:** Jane Heng
    **Goal:** Demonstrate systematic model optimization through iterative improvements
    **Key Innovation:** 4-step optimization achieving +13.9% accuracy improvement from baseline
    """)

    # Prepare data for health risk classification
    X = df[FEATURE_COLS].dropna()
    y_obesity = df.loc[X.index, '% Adults with Obesity']

    # Create binary health risk target (median split)
    median_obesity = y_obesity.median()
    y_risk = (y_obesity > median_obesity).astype(int)

    X_train, X_test, y_train, y_test = train_test_split(X, y_risk, test_size=0.3, random_state=42, stratify=y_risk)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    tabs = st.tabs(["Optimization Overview", "Step 1: Three-Class Baseline", "Step 2: Binary Classification",
                    "Step 3: DBSCAN Filtering", "Step 4: Ensemble Methods", "Model Evolution"])

    with tabs[0]:
        st.markdown("### 4-Step Optimization Pipeline")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            <div class="metric-card">
            <h4>Optimization Journey</h4>
            <p><strong>Step 1:</strong> Three-Class Classification (Baseline)</p>
            <p>‚Üí Accuracy: 55.3%</p>
            <p><strong>Step 2:</strong> Binary Classification</p>
            <p>‚Üí Accuracy: 66.0% (+10.7%)</p>
            <p><strong>Step 3:</strong> DBSCAN Noise Filtering</p>
            <p>‚Üí Accuracy: 66.8% (+0.7%)</p>
            <p><strong>Step 4:</strong> Ensemble Methods (Stacking)</p>
            <p>‚Üí Accuracy: 69.2% (+2.4%)</p>
            <p><strong>Total Improvement: +13.9 percentage points</strong></p>
            </div>
            """, unsafe_allow_html=True)

        with col2:
            st.markdown("""
            <div class="insight-box">
            <h4>Key Insights</h4>
            <p>‚úì Binary classification significantly outperforms three-class (10.7% gain)</p>
            <p>‚úì Data quality matters: DBSCAN noise removal provides 0.7% improvement</p>
            <p>‚úì Ensemble methods capture complex patterns (+2.4%)</p>
            <p>‚úì Food Access Barrier Index ranks #3 in feature importance (12.21%)</p>
            <p>‚úì Systematic optimization > random hyperparameter tuning</p>
            </div>
            """, unsafe_allow_html=True)

        # Model evolution visualization
        st.markdown("### Model Performance Evolution")

        stages = ['Step 1:\nThree-Class', 'Step 2:\nBinary', 'Step 3:\nDBSCAN', 'Step 4:\nEnsemble']
        accuracies = [0.553, 0.660, 0.668, 0.692]
        improvements = [0, 10.7, 0.7, 2.4]

        fig = go.Figure()

        fig.add_trace(go.Scatter(
            x=stages,
            y=accuracies,
            mode='lines+markers+text',
            name='Accuracy',
            line=dict(color='#1e88e5', width=3),
            marker=dict(size=12),
            text=[f'{acc:.1%}' for acc in accuracies],
            textposition='top center'
        ))

        for i in range(1, len(stages)):
            fig.add_annotation(
                x=i, y=accuracies[i],
                text=f'+{improvements[i]:.1f}%',
                showarrow=True,
                arrowhead=2,
                arrowcolor='green',
                ax=-40, ay=-30,
                bgcolor='rgba(76, 175, 80, 0.2)',
                bordercolor='green'
            )

        fig.update_layout(
            title='Optimization Pipeline: Progressive Accuracy Improvement',
            xaxis_title='Optimization Stage',
            yaxis_title='Accuracy',
            yaxis=dict(range=[0.5, 0.75], tickformat='.0%'),
            height=400,
            showlegend=False
        )

        st.plotly_chart(fig, use_container_width=True)

    with tabs[1]:
        st.markdown("### Step 1: Three-Class Classification (Baseline)")

        st.markdown("""
        **Approach:** Classify counties into High/Medium/Low health risk categories

        **Problem:** The "Medium" class creates ambiguous boundaries, leading to poor performance

        **Gini Index Analysis** shows that three-class classification has lower class purity (0.58)
        compared to binary classification (0.64), indicating overlapping/ambiguous classes.
        """)

        # Show sample three-class distribution
        st.markdown("**Health Risk Score Distribution (Three Classes)**")
        health_scores = df['Health_Risk_Score'].dropna()

        fig = px.histogram(
            health_scores,
            nbins=50,
            title='Health Risk Score Distribution with Three-Class Boundaries',
            labels={'value': 'Health Risk Score', 'count': 'Number of Counties'},
            color_discrete_sequence=['#1e88e5']
        )

        # Add threshold lines
        low_threshold = health_scores.quantile(0.33)
        high_threshold = health_scores.quantile(0.67)

        fig.add_vline(x=low_threshold, line_dash="dash", line_color="orange",
                     annotation_text="Low/Medium")
        fig.add_vline(x=high_threshold, line_dash="dash", line_color="red",
                     annotation_text="Medium/High")

        st.plotly_chart(fig, use_container_width=True)

        st.markdown("""
        <div class="warning-box">
        <strong>Lesson Learned:</strong> The "Medium" class (between 33rd and 67th percentiles) contains
        counties with ambiguous characteristics, making it difficult for models to learn clear decision boundaries.
        This results in baseline accuracy of only 55.3%.
        </div>
        """, unsafe_allow_html=True)

    with tabs[2]:
        st.markdown("### Step 2: Binary Classification")

        st.markdown("""
        **Approach:** Simplify to High vs. Low risk using median obesity rate as threshold

        **Result:** +10.7 percentage point improvement (55.3% ‚Üí 66.0%)

        **Why it works:** Binary classification creates clearer decision boundaries with less class overlap
        """)

        # Train baseline binary models
        rf_baseline = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
        et_baseline = ExtraTreesClassifier(n_estimators=100, max_depth=5, random_state=42)

        rf_baseline.fit(X_train_scaled, y_train)
        et_baseline.fit(X_train_scaled, y_train)

        rf_pred = rf_baseline.predict(X_test_scaled)
        et_pred = et_baseline.predict(X_test_scaled)

        rf_acc = accuracy_score(y_test, rf_pred)
        et_acc = accuracy_score(y_test, et_pred)

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("Random Forest Accuracy", f"{rf_acc:.1%}")
        with col2:
            st.metric("Extra Trees Accuracy", f"{et_acc:.1%}")
        with col3:
            st.metric("Improvement from Step 1", "+10.7%", delta="10.7%")

        # Confusion matrices
        st.markdown("**Confusion Matrices**")

        fig, axes = plt.subplots(1, 2, figsize=(12, 5))

        cm_rf = confusion_matrix(y_test, rf_pred)
        cm_et = confusion_matrix(y_test, et_pred)

        sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0],
                   xticklabels=['Low Risk', 'High Risk'],
                   yticklabels=['Low Risk', 'High Risk'])
        axes[0].set_title('Random Forest')
        axes[0].set_ylabel('Actual')
        axes[0].set_xlabel('Predicted')

        sns.heatmap(cm_et, annot=True, fmt='d', cmap='Greens', ax=axes[1],
                   xticklabels=['Low Risk', 'High Risk'],
                   yticklabels=['Low Risk', 'High Risk'])
        axes[1].set_title('Extra Trees')
        axes[1].set_ylabel('Actual')
        axes[1].set_xlabel('Predicted')

        plt.tight_layout()
        st.pyplot(fig)
        plt.clf()

    with tabs[3]:
        st.markdown("### Step 3: DBSCAN Noise Filtering")

        st.markdown("""
        **Approach:** Use DBSCAN clustering to identify and remove outlier/noisy samples from training data

        **Result:** +0.7 percentage point improvement (66.0% ‚Üí 66.8%)

        **Key Insight:** Even small data quality improvements matter
        """)

        # Apply DBSCAN
        dbscan = DBSCAN(eps=2.0, min_samples=10)
        cluster_labels = dbscan.fit_predict(X_train_scaled)

        noise_mask = cluster_labels == -1
        clean_mask = ~noise_mask

        X_train_clean = X_train_scaled[clean_mask]
        y_train_clean = y_train[clean_mask]

        st.markdown(f"""
        **DBSCAN Configuration:**
        - Epsilon (eps): 2.0
        - Min samples: 10
        - Noise samples detected: {noise_mask.sum()} ({noise_mask.sum()/len(y_train)*100:.1f}% of training data)
        - Clean samples retained: {clean_mask.sum()}
        """)

        # Train models on cleaned data
        rf_clean = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
        et_clean = ExtraTreesClassifier(n_estimators=100, max_depth=5, random_state=42)

        rf_clean.fit(X_train_clean, y_train_clean)
        et_clean.fit(X_train_clean, y_train_clean)

        rf_clean_pred = rf_clean.predict(X_test_scaled)
        et_clean_pred = et_clean.predict(X_test_scaled)

        rf_clean_acc = accuracy_score(y_test, rf_clean_pred)
        et_clean_acc = accuracy_score(y_test, et_clean_pred)

        # Comparison table
        comparison_data = {
            'Model': ['Random Forest', 'Extra Trees'],
            'Before DBSCAN': [rf_acc, et_acc],
            'After DBSCAN': [rf_clean_acc, et_clean_acc],
            'Improvement': [rf_clean_acc - rf_acc, et_clean_acc - et_acc]
        }

        comparison_df = pd.DataFrame(comparison_data)
        comparison_df['Before DBSCAN'] = comparison_df['Before DBSCAN'].apply(lambda x: f'{x:.4f}')
        comparison_df['After DBSCAN'] = comparison_df['After DBSCAN'].apply(lambda x: f'{x:.4f}')
        comparison_df['Improvement'] = comparison_df['Improvement'].apply(lambda x: f'+{x:.4f}' if x > 0 else f'{x:.4f}')

        st.dataframe(comparison_df, use_container_width=True)

        # Visualize noise points using PCA
        st.markdown("**DBSCAN Noise Detection Visualization (2D PCA)**")

        pca_vis = PCA(n_components=2)
        X_pca = pca_vis.fit_transform(X_train_scaled)

        fig = plt.figure(figsize=(10, 6))
        scatter_clean = plt.scatter(X_pca[clean_mask, 0], X_pca[clean_mask, 1],
                                   c=y_train[clean_mask], cmap='RdYlGn_r',
                                   alpha=0.6, s=30, label='Clean samples', edgecolors='black', linewidth=0.5)
        scatter_noise = plt.scatter(X_pca[noise_mask, 0], X_pca[noise_mask, 1],
                                   c='red', marker='x', s=100, label='Noise (outliers)', linewidths=2)

        plt.xlabel(f'PC1 ({pca_vis.explained_variance_ratio_[0]*100:.1f}% variance)')
        plt.ylabel(f'PC2 ({pca_vis.explained_variance_ratio_[1]*100:.1f}% variance)')
        plt.title('DBSCAN Noise Detection in Feature Space')
        plt.colorbar(scatter_clean, label='Health Risk')
        plt.legend()
        plt.grid(alpha=0.3)
        st.pyplot(fig)
        plt.clf()

    with tabs[4]:
        st.markdown("### Step 4: Ensemble Methods")

        st.markdown("""
        **Approach:** Combine multiple models using advanced ensemble techniques

        **Result:** +2.4 percentage point improvement (66.8% ‚Üí 69.2%)

        **Winner:** Stacking Classifier (69.2% accuracy, 0.7607 AUC-ROC)
        """)

        # Train ensemble models on DBSCAN-cleaned data
        st.markdown("**Training Ensemble Models...**")

        # Random Forest
        rf_ensemble = RandomForestClassifier(n_estimators=100, random_state=42)
        rf_ensemble.fit(X_train_clean, y_train_clean)
        rf_ensemble_acc = accuracy_score(y_test, rf_ensemble.predict(X_test_scaled))

        # Gradient Boosting
        gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
        gb.fit(X_train_clean, y_train_clean)
        gb_acc = accuracy_score(y_test, gb.predict(X_test_scaled))

        # Bagging
        bagging = BaggingClassifier(estimator=RandomForestClassifier(n_estimators=50, random_state=42),
                                    n_estimators=10, random_state=42)
        bagging.fit(X_train_clean, y_train_clean)
        bagging_acc = accuracy_score(y_test, bagging.predict(X_test_scaled))

        # Voting Classifier
        rf_vote = RandomForestClassifier(n_estimators=100, random_state=42)
        et_vote = ExtraTreesClassifier(n_estimators=100, random_state=42)
        gb_vote = GradientBoostingClassifier(n_estimators=100, random_state=42)

        voting = VotingClassifier(
            estimators=[('rf', rf_vote), ('et', et_vote), ('gb', gb_vote)],
            voting='soft'
        )
        voting.fit(X_train_clean, y_train_clean)
        voting_acc = accuracy_score(y_test, voting.predict(X_test_scaled))

        # Stacking Classifier (BEST)
        base_learners = [
            ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),
            ('et', ExtraTreesClassifier(n_estimators=100, random_state=42)),
            ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))
        ]

        stacking = StackingClassifier(
            estimators=base_learners,
            final_estimator=LogisticRegression(max_iter=1000),
            cv=5
        )
        stacking.fit(X_train_clean, y_train_clean)
        stacking_pred = stacking.predict(X_test_scaled)
        stacking_acc = accuracy_score(y_test, stacking_pred)
        stacking_proba = stacking.predict_proba(X_test_scaled)[:, 1]

        from sklearn.metrics import roc_auc_score
        stacking_auc = roc_auc_score(y_test, stacking_proba)

        # Ensemble comparison
        ensemble_data = {
            'Method': ['Random Forest', 'Gradient Boosting', 'Bagging', 'Voting (Soft)', 'Stacking ‚≠ê'],
            'Accuracy': [rf_ensemble_acc, gb_acc, bagging_acc, voting_acc, stacking_acc],
            'Description': [
                'Single ensemble baseline',
                'Sequential tree boosting',
                'Bootstrap aggregating',
                'Soft voting of 3 models',
                'Meta-learner on base predictions'
            ]
        }

        ensemble_df = pd.DataFrame(ensemble_data)
        ensemble_df = ensemble_df.sort_values('Accuracy', ascending=False)

        st.dataframe(
            ensemble_df.style.format({'Accuracy': '{:.4f}'})
                           .background_gradient(subset=['Accuracy'], cmap='YlGn'),
            use_container_width=True
        )

        # Highlight best model
        st.markdown(f"""
        <div class="insight-box">
        <h4>üèÜ Best Model: Stacking Classifier</h4>
        <p><strong>Accuracy:</strong> {stacking_acc:.4f} ({stacking_acc*100:.1f}%)</p>
        <p><strong>ROC-AUC:</strong> {stacking_auc:.4f}</p>
        <p><strong>Improvement from Step 1:</strong> +{(stacking_acc - 0.553)*100:.1f} percentage points</p>
        <p><strong>Architecture:</strong> 3 base learners (RF, ET, GB) + Logistic Regression meta-learner</p>
        </div>
        """, unsafe_allow_html=True)

        # Confusion matrix for stacking
        st.markdown("**Stacking Classifier Confusion Matrix**")

        cm_stacking = confusion_matrix(y_test, stacking_pred)

        fig = plt.figure(figsize=(8, 6))
        sns.heatmap(cm_stacking, annot=True, fmt='d', cmap='RdYlGn_r',
                   xticklabels=['Low Risk', 'High Risk'],
                   yticklabels=['Low Risk', 'High Risk'])
        plt.title('Stacking Classifier - Final Model Performance')
        plt.ylabel('Actual')
        plt.xlabel('Predicted')
        st.pyplot(fig)
        plt.clf()

        # Feature importance from base RF model
        st.markdown("**Feature Importance (Random Forest Base Learner)**")

        feature_importance = pd.DataFrame({
            'Feature': FEATURE_COLS,
            'Importance': rf_ensemble.feature_importances_
        }).sort_values('Importance', ascending=False)

        fig = px.bar(feature_importance.head(10), x='Importance', y='Feature', orientation='h',
                    title='Top 10 Most Important Features',
                    color='Importance', color_continuous_scale='Viridis')
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)

        # Highlight food desert ranking
        food_feature_mask = feature_importance['Feature'] == 'Food_Access_Barrier_Index'
        if food_feature_mask.any():
            food_rank = feature_importance[food_feature_mask].index[0] + 1
            food_importance = feature_importance[food_feature_mask]['Importance'].values[0]

            st.markdown(f"""
            <div class="insight-box">
            <strong>Food Desert Impact:</strong> Food_Access_Barrier_Index ranks #{food_rank}
            with {food_importance*100:.2f}% importance, demonstrating independent contribution to health risk
            prediction even after controlling for socioeconomic factors.
            </div>
            """, unsafe_allow_html=True)
        else:
            st.markdown("""
            <div class="insight-box">
            <strong>Note:</strong> Food_Access_Barrier_Index not included in current feature set.
            The model uses a comprehensive set of socioeconomic and health factors for prediction.
            </div>
            """, unsafe_allow_html=True)

    with tabs[5]:
        st.markdown("### Complete Model Evolution Summary")

        # Summary table
        summary_data = {
            'Stage': ['Step 1: Three-Class', 'Step 2: Binary', 'Step 3: DBSCAN', 'Step 4: Ensemble'],
            'Best Accuracy': [0.553, 0.660, 0.668, 0.692],
            'Improvement': ['-', '+10.7%', '+0.7%', '+2.4%'],
            'Key Innovation': [
                'Baseline with ambiguous classes',
                'Simplified to binary classification',
                'Data quality improvement via outlier removal',
                'Stacking ensemble with meta-learner'
            ]
        }

        summary_df = pd.DataFrame(summary_data)

        st.dataframe(
            summary_df.style.format({'Best Accuracy': '{:.3f}'}),
            use_container_width=True
        )

        # Cumulative improvement visualization
        st.markdown("**Cumulative Accuracy Improvement**")

        stages = ['Baseline', 'Binary', '+ DBSCAN', '+ Ensemble']
        cumulative_acc = [0.553, 0.660, 0.668, 0.692]
        improvements_cum = [0, 10.7, 11.5, 13.9]

        fig = make_subplots(specs=[[{"secondary_y": False}]])

        fig.add_trace(
            go.Bar(x=stages, y=cumulative_acc, name='Accuracy',
                  text=[f'{acc:.1%}' for acc in cumulative_acc],
                  textposition='outside',
                  marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
        )

        fig.update_layout(
            title='Model Optimization Journey: From 55.3% to 69.2%',
            xaxis_title='Optimization Stage',
            yaxis_title='Accuracy',
            yaxis=dict(range=[0.5, 0.75], tickformat='.0%'),
            height=400,
            showlegend=False
        )

        st.plotly_chart(fig, use_container_width=True)

        st.markdown("""
        ### Key Takeaways

        1. **Problem formulation matters most:** Binary vs three-class classification decision yielded
           largest single improvement (+10.7%)

        2. **Data quality is crucial:** Even removing 3-4% noisy samples improved performance by 0.7%

        3. **Ensemble methods capture complexity:** Stacking combines strengths of multiple algorithms
           for additional 2.4% gain

        4. **Systematic optimization > random tuning:** Following a structured pipeline yielded
           13.9 percentage point improvement

        5. **Food desert relevance confirmed:** Food_Access_Barrier_Index maintains top-3 feature
           importance throughout optimization, validating project focus
        """)

        st.markdown("""
        <div class="warning-box">
        <strong>Methodological Note:</strong> All improvements were validated on held-out test data
        that was never used during training or hyperparameter selection. Cross-validation was used
        within the Stacking meta-learner to prevent overfitting.
        </div>
        """, unsafe_allow_html=True)

# =============================================================================
# PAGE 6: Clustering Analysis
# =============================================================================
elif page == "6. Clustering Analysis":
    st.markdown('<div class="main-header">Clustering Analysis</div>', unsafe_allow_html=True)

    st.markdown("**Goal:** Identify distinct county profiles based on health and socioeconomic indicators")

    # Prepare data
    cluster_features = ['% Adults with Obesity', '% Adults with Diabetes',
                        'Food Environment Index', 'Income Ratio', '% Children in Poverty']
    X_cluster = df[cluster_features].dropna()

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_cluster)

    tab1, tab2, tab3 = st.tabs(["Algorithm Details", "K-Means Analysis", "Cluster Profiles"])

    with tab1:
        st.markdown("### K-Means Clustering Overview")

        st.markdown("""
        We use K-Means clustering to identify distinct county health profiles based on:
        - Health outcomes (obesity, diabetes)
        - Socioeconomic factors (poverty, education, income inequality)
        - Food environment (food access barrier index)

        **Why K-Means only?**
        - Fast and scalable (O(nk) vs O(n¬≤) for hierarchical)
        - Clear, interpretable cluster centers
        - Silhouette score of 0.44 indicates well-separated clusters
        - Hierarchical clustering showed 83% agreement with K-Means but was significantly slower
        - Dendrogram didn't provide additional actionable insights for policy recommendations
        """)

        st.markdown("""
        <div class="metric-card">
        <h4>K-Means Clustering</h4>
        <p><strong>Goal:</strong> Partition counties into k groups minimizing within-cluster variance</p>
        <p><strong>Algorithm:</strong> Iteratively assign points to nearest centroid, update centroids until convergence</p>
        <p><strong>Hyperparameters:</strong></p>
        <ul>
        <li><strong>k</strong>: Number of clusters (we chose k=5 based on elbow method)</li>
        <li><strong>init</strong>: k-means++ for smart initialization</li>
        <li><strong>n_init</strong>: 10 runs with different seeds to avoid local minima</li>
        </ul>
        <p><strong>Pros:</strong> Fast (O(nk)), scalable to large datasets, easy to interpret cluster centers</p>
        <p><strong>Cons:</strong> Assumes spherical clusters, sensitive to outliers, must specify k</p>
        <p><strong>Results:</strong> Identified 5 distinct county profiles from healthiest (Cluster 1) to highest risk (Cluster 4)</p>
        </div>
        """, unsafe_allow_html=True)

        st.markdown("### Evaluation Metrics")
        st.markdown("""
        | Metric | Range | Interpretation |
        |--------|-------|----------------|
        | **Inertia** | 0 to ‚àû | Within-cluster sum of squares (lower = tighter clusters) |
        | **Silhouette Score** | -1 to 1 | Cohesion vs separation (higher = better defined clusters) |
        | **Elbow Point** | - | k where adding more clusters gives diminishing returns |
        """)

        st.markdown("""
        <div class="insight-box">
        <strong>How to choose k:</strong><br>
        1. <strong>Elbow Method:</strong> Look for "bend" in inertia plot<br>
        2. <strong>Silhouette:</strong> Maximize silhouette score<br>
        3. <strong>Domain Knowledge:</strong> What makes sense for your problem?
        </div>
        """, unsafe_allow_html=True)

    with tab2:
        st.markdown("### K-Means Clustering")

        # Elbow method
        inertias = []
        silhouettes = []
        K_range = range(2, 11)

        for k in K_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            kmeans.fit(X_scaled)
            inertias.append(kmeans.inertia_)
            silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))

        col1, col2 = st.columns(2)

        with col1:
            fig = px.line(x=list(K_range), y=inertias, markers=True,
                          title='Elbow Method', labels={'x': 'k', 'y': 'Inertia'})
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            fig = px.line(x=list(K_range), y=silhouettes, markers=True,
                          title='Silhouette Scores', labels={'x': 'k', 'y': 'Score'})
            st.plotly_chart(fig, use_container_width=True)

        # Final clustering with k=5
        n_clusters = st.slider("Number of Clusters", 2, 10, 5)
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(X_scaled)

        st.metric("Silhouette Score", f"{silhouette_score(X_scaled, clusters):.4f}")

        # Visualize with PCA
        pca = PCA(n_components=2)
        X_pca = pca.fit_transform(X_scaled)

        fig = px.scatter(x=X_pca[:, 0], y=X_pca[:, 1], color=clusters.astype(str),
                         title=f'K-Means Clusters (k={n_clusters}) - PCA Projection',
                         labels={'x': 'PC1', 'y': 'PC2', 'color': 'Cluster'})
        st.plotly_chart(fig, use_container_width=True)

        st.markdown(f"""
        <div class="insight-box">
        <strong>K-Means Interpretation:</strong><br>
        ‚Ä¢ Silhouette > 0.5 = good cluster structure<br>
        ‚Ä¢ Silhouette 0.25-0.5 = moderate structure<br>
        ‚Ä¢ Current score indicates {'well-defined' if silhouette_score(X_scaled, clusters) > 0.3 else 'moderate'} cluster separation
        </div>
        """, unsafe_allow_html=True)

    with tab3:
        st.markdown("### Cluster Profiles (K-Means, k=5)")

        kmeans_5 = KMeans(n_clusters=5, random_state=42, n_init=10)
        df_temp = X_cluster.copy()
        df_temp['Cluster'] = kmeans_5.fit_predict(X_scaled)

        # Cluster means
        cluster_profiles = df_temp.groupby('Cluster').mean().round(2)
        cluster_profiles['Count'] = df_temp.groupby('Cluster').size()

        st.dataframe(cluster_profiles, use_container_width=True)

        # Radar chart
        fig = go.Figure()

        for cluster in range(5):
            values = cluster_profiles.loc[cluster, cluster_features].values
            values = np.append(values, values[0])  # Close the polygon

            fig.add_trace(go.Scatterpolar(
                r=values,
                theta=cluster_features + [cluster_features[0]],
                fill='toself',
                name=f'Cluster {cluster}'
            ))

        fig.update_layout(polar=dict(radialaxis=dict(visible=True)),
                          title='Cluster Profiles (Standardized)',
                          height=500)
        st.plotly_chart(fig, use_container_width=True)

        st.markdown("### Cluster Interpretation")

        # Add cluster interpretations
        st.markdown("""
        | Cluster | Profile | Characteristics | Policy Priority |
        |---------|---------|-----------------|-----------------|
        | **0** | Moderate Risk | Average health, moderate food access | Monitor |
        | **1** | Healthiest | Low obesity/diabetes, good food access | Maintain |
        | **2** | High Risk | High poverty, poor food environment | High Priority |
        | **3** | Mixed | Lower obesity but moderate poverty | Medium Priority |
        | **4** | Highest Risk | Worst health outcomes, poorest food access | **Urgent** |
        """)

        st.markdown("""
        <div class="insight-box">
        <strong>Key Finding:</strong><br>
        ‚Ä¢ <strong>Cluster 4</strong> (highest risk) shows 42% obesity, 14% diabetes, and Food Index of 6.5<br>
        ‚Ä¢ <strong>Cluster 1</strong> (healthiest) has 33% obesity, 9% diabetes, and Food Index of 8.3<br>
        ‚Ä¢ <strong>Gap:</strong> 9 percentage points in obesity, 5 points in diabetes between best and worst clusters<br>
        ‚Ä¢ <strong>Target:</strong> Cluster 4 counties for food access interventions
        </div>
        """, unsafe_allow_html=True)

# =============================================================================
# PAGE 7: PCA Analysis
# =============================================================================
elif page == "7. PCA Analysis":
    st.markdown('<div class="main-header">Principal Component Analysis</div>', unsafe_allow_html=True)

    st.markdown("**Goal:** Reduce dimensionality while preserving maximum variance, identify latent patterns")

    # Prepare data
    pca_features = ['% Adults with Obesity', '% Adults with Diabetes',
                    'Food Environment Index', '% Children in Poverty',
                    '% Completed High School', 'Income Ratio', '% Uninsured',
                    '% Rural', '% Excessive Drinking', '% Insufficient Sleep']

    X_pca = df[pca_features].dropna()

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_pca)

    # Fit PCA
    pca = PCA()
    X_transformed = pca.fit_transform(X_scaled)

    tab1, tab2, tab3, tab4 = st.tabs(["Algorithm Details", "Variance Explained", "Component Loadings", "3D Visualization"])

    with tab1:
        st.markdown("### PCA Algorithm Overview")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("""
            <div class="metric-card">
            <h4>Principal Component Analysis</h4>
            <p><strong>Goal:</strong> Find orthogonal axes that maximize variance</p>
            <p><strong>Algorithm:</strong></p>
            <ol>
            <li>Standardize features (mean=0, std=1)</li>
            <li>Compute covariance matrix</li>
            <li>Find eigenvalues & eigenvectors</li>
            <li>Sort by eigenvalue (variance explained)</li>
            <li>Project data onto top k components</li>
            </ol>
            <p><strong>Parameters:</strong> None (unsupervised)</p>
            <p><strong>Hyperparameters:</strong> n_components</p>
            </div>
            """, unsafe_allow_html=True)

        with col2:
            st.markdown("""
            <div class="metric-card">
            <h4>Key Concepts</h4>
            <p><strong>Loadings:</strong> Correlation between original features and PCs</p>
            <p><strong>Scores:</strong> Data projected onto PC space</p>
            <p><strong>Eigenvalue:</strong> Variance explained by each PC</p>
            <p><strong>Explained Variance Ratio:</strong> % of total variance per PC</p>
            <br>
            <p><strong>Use Cases:</strong></p>
            <ul>
            <li>Dimensionality reduction</li>
            <li>Visualization (2D/3D)</li>
            <li>Noise reduction</li>
            <li>Feature extraction</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)

        st.markdown("### How to Interpret")
        st.markdown("""
        | Element | Interpretation |
        |---------|----------------|
        | **Scree Plot** | Elbow = stop adding components |
        | **Cumulative Variance** | 80-90% = good threshold |
        | **High Loading (>0.5)** | Feature strongly contributes to PC |
        | **Similar Loadings** | Features are correlated |
        | **Opposite Signs** | Features are inversely related |
        """)

    with tab2:
        st.markdown("### Scree Plot & Cumulative Variance")

        explained_var = pca.explained_variance_ratio_
        cumulative_var = np.cumsum(explained_var)

        col1, col2 = st.columns(2)

        with col1:
            fig = px.bar(x=[f'PC{i+1}' for i in range(len(explained_var))],
                         y=explained_var * 100,
                         title='Variance Explained by Component',
                         labels={'x': 'Principal Component', 'y': 'Variance Explained (%)'})
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            fig = px.line(x=[f'PC{i+1}' for i in range(len(cumulative_var))],
                          y=cumulative_var * 100, markers=True,
                          title='Cumulative Variance Explained',
                          labels={'x': 'Principal Component', 'y': 'Cumulative Variance (%)'})
            fig.add_hline(y=80, line_dash='dash', line_color='red',
                          annotation_text='80% threshold')
            st.plotly_chart(fig, use_container_width=True)

        # Components needed for 80%
        n_components_80 = np.argmax(cumulative_var >= 0.80) + 1
        st.markdown(f'<div class="insight-box"><strong>{n_components_80} components</strong> explain 80% of variance, reducing from {len(pca_features)} features.</div>', unsafe_allow_html=True)

    with tab3:
        st.markdown("### PCA Loadings Heatmap")

        loadings = pd.DataFrame(
            pca.components_.T,
            columns=[f'PC{i+1}' for i in range(len(pca_features))],
            index=pca_features
        )

        n_pcs = st.slider("Number of PCs to Display", 2, len(pca_features), 5)

        fig = px.imshow(loadings.iloc[:, :n_pcs],
                        labels=dict(color="Loading"),
                        color_continuous_scale='RdBu_r',
                        aspect='auto',
                        title=f'Feature Loadings on First {n_pcs} PCs')
        fig.update_layout(height=500)
        st.plotly_chart(fig, use_container_width=True)

        st.markdown("### Top Loadings by Component")
        pc_choice = st.selectbox("Select PC", [f'PC{i+1}' for i in range(n_pcs)])
        top_loadings = loadings[pc_choice].abs().sort_values(ascending=False)
        st.dataframe(top_loadings.round(3), use_container_width=True)

        st.markdown("""
        <div class="insight-box">
        <strong>Loadings Interpretation:</strong><br>
        ‚Ä¢ <strong>PC1:</strong> Overall health/socioeconomic status (poverty, education, health outcomes load together)<br>
        ‚Ä¢ <strong>PC2:</strong> Urban-rural divide and lifestyle factors<br>
        ‚Ä¢ Features with similar loadings are correlated in the data
        </div>
        """, unsafe_allow_html=True)

    with tab4:
        st.markdown("### 3D PCA Projection")

        # Color by Area Type
        df_vis = df.loc[X_pca.index].copy()
        df_vis['PC1'] = X_transformed[:, 0]
        df_vis['PC2'] = X_transformed[:, 1]
        df_vis['PC3'] = X_transformed[:, 2]

        fig = px.scatter_3d(df_vis, x='PC1', y='PC2', z='PC3',
                            color='Area_Type',
                            hover_data=['State', 'County', '% Adults with Obesity'],
                            title='Counties in PC Space (colored by Area Type)')
        fig.update_layout(height=600)
        st.plotly_chart(fig, use_container_width=True)

# =============================================================================
# PAGE 7: Model Comparison
# =============================================================================
elif page == "8. Model Comparison":
    st.markdown('<div class="main-header">Model Comparison & Evaluation</div>', unsafe_allow_html=True)

    tab1, tab2 = st.tabs(["Regression Comparison", "Classification Comparison"])

    with tab1:
        st.markdown("### Regression Models Summary")

        # Prepare data
        X = df[FEATURE_COLS].dropna()
        y = df.loc[X.index, TARGET_REGRESSION]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        reg_models = {
            'Linear Regression': LinearRegression(),
            'Ridge': Ridge(alpha=1.0)
        }

        reg_results = []
        for name, model in reg_models.items():
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)

            reg_results.append({
                'Model': name,
                'R¬≤': r2_score(y_test, y_pred),
                'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
                'MAE': mean_absolute_error(y_test, y_pred)
            })

        reg_df = pd.DataFrame(reg_results)

        fig = go.Figure(data=[
            go.Bar(name='R¬≤', x=reg_df['Model'], y=reg_df['R¬≤']),
            go.Bar(name='RMSE/10', x=reg_df['Model'], y=reg_df['RMSE']/10),
            go.Bar(name='MAE/10', x=reg_df['Model'], y=reg_df['MAE']/10)
        ])
        fig.update_layout(barmode='group', title='Regression Metrics')
        st.plotly_chart(fig, use_container_width=True)

        st.dataframe(reg_df.round(4), use_container_width=True)

        best_reg = reg_df.loc[reg_df['R¬≤'].idxmax(), 'Model']
        st.markdown(f'<div class="insight-box"><strong>Best Regression Model:</strong> {best_reg}</div>', unsafe_allow_html=True)

    with tab2:
        st.markdown("### Classification Models Summary")

        # Prepare data
        X = df[FEATURE_COLS].dropna()
        y = df.loc[X.index, TARGET_CLASSIFICATION]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        clf_models = {
            'Logistic Regression': LogisticRegression(max_iter=1000),
            'SVM': SVC(kernel='rbf'),
            'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),
            'Extra Trees': ExtraTreesClassifier(n_estimators=100, max_depth=5, random_state=42)
        }

        clf_results = []
        for name, model in clf_models.items():
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)

            clf_results.append({
                'Model': name,
                'Accuracy': accuracy_score(y_test, y_pred),
                'Precision': precision_score(y_test, y_pred),
                'Recall': recall_score(y_test, y_pred),
                'F1 Score': f1_score(y_test, y_pred)
            })

        clf_df = pd.DataFrame(clf_results).sort_values('F1 Score', ascending=False)

        fig = px.bar(clf_df, x='Model', y=['Accuracy', 'Precision', 'Recall', 'F1 Score'],
                     barmode='group', title='Classification Metrics Comparison')
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)

        st.dataframe(clf_df.round(4), use_container_width=True)

        best_clf = clf_df.iloc[0]['Model']
        st.markdown(f'<div class="insight-box"><strong>Best Classification Model:</strong> {best_clf} with F1={clf_df.iloc[0]["F1 Score"]:.4f}</div>', unsafe_allow_html=True)

# =============================================================================
# PAGE 9: Conclusions
# =============================================================================
elif page == "9. Key Insights & Conclusions":
    st.markdown('<div class="main-header">Key Insights & Conclusions</div>', unsafe_allow_html=True)

    st.markdown("## Summary of Findings")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### Data Insights")
        st.markdown("""
        - **2,275 counties** analyzed across 48 states
        - **74% rural** counties in dataset
        - Strong correlation between **food access** and health outcomes
        - **Income inequality** linked to worse health metrics
        """)

        st.markdown("### Best Performing Models")
        st.markdown("""
        **Regression (Obesity Prediction):**
        - Ridge outperforms Linear (R¬≤ = 0.417 vs 0.411)
        - Moderate fit (~42% variance explained)
        - Key predictors: Sleep Deprivation, Drinking, Food Environment

        **Classification (Income Inequality):**
        - Random Forest achieves best F1 = 0.848
        - Extra Trees close second (F1 = 0.843)
        - Progression: Logistic (0.815) ‚Üí SVM (0.809) ‚Üí Ensembles (0.84+)
        - Advanced stacking ensemble reaches 69.2% accuracy
        """)

    with col2:
        st.markdown("### Clustering Results")
        st.markdown("""
        - **5 distinct county profiles** identified
        - Cluster 4: Highest risk (poor food access, high poverty)
        - Cluster 1: Healthiest (good food environment, low poverty)
        - Silhouette score ~ 0.25 (moderate separation)
        """)

        st.markdown("### PCA Results")
        st.markdown("""
        - **4-5 components** capture 80% of variance
        - PC1: Overall health/socioeconomic status
        - PC2: Urban-rural divide
        - Useful for visualization and noise reduction
        """)

    st.markdown("---")

    st.markdown("## Policy Recommendations")

    rec_col1, rec_col2, rec_col3 = st.columns(3)

    with rec_col1:
        st.markdown("### Food Access")
        st.markdown("""
        - Incentivize supermarkets in food deserts
        - Support farmers markets & SNAP
        - Mobile food programs
        """)

    with rec_col2:
        st.markdown("### Health Behaviors")
        st.markdown("""
        - Sleep health education
        - Alcohol intervention programs
        - Physical activity infrastructure
        """)

    with rec_col3:
        st.markdown("### Socioeconomic")
        st.markdown("""
        - Target high-inequality counties
        - Job training programs
        - Healthcare access expansion
        """)

    st.markdown("---")

    st.markdown("## Limitations & Future Work")
    st.markdown("""
    - **Cross-sectional data**: Cannot establish causality
    - **County-level aggregation**: Masks within-county variation
    - **Future**: Time-series analysis, finer geographic resolution, additional health outcomes
    """)

    st.markdown("---")
    st.markdown("### DATA-245 Machine Learning | Group 3")
    st.markdown("*Savitha, Jane Heng, Rishi Boppana, Kapil Sanikommu*")
